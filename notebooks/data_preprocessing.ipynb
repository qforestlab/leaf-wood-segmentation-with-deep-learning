{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Data preprocessing\n",
    "\n",
    "\n",
    "Code used for preprocessing the dataset presented in the paper \"Pointwise deep learning for leaf-wood segmentation of tropical tree point clouds from terrestrial laser scanning\"  \n",
    "\n",
    "--- "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Jupyter environment detected. Enabling Open3D WebVisualizer.\n",
      "[Open3D INFO] WebRTC GUI backend enabled.\n",
      "[Open3D INFO] WebRTCWindowSystem: HTTP handshake server disabled.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import open3d as o3d\n",
    "# import open3d.ml as _ml3d\n",
    "# import open3d.ml.torch as ml3d\n",
    "import ml3d as _ml3d\n",
    "import ml3d.torch as ml3d\n",
    "from open3d.ml.torch.datasets import Custom3D\n",
    "import numpy as np\n",
    "import glob\n",
    "import torch\n",
    "from pclbox.models import CustomRandLANet\n",
    "\n",
    "DATA_DIR = \"/mnt/c/Users/wavdnbro/OneDrive - UGent/Documents/spacetwin/datasets/leaf_wood/\"\n",
    "DATA_PATH = DATA_DIR + 'preprocessed/'"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocess dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------------\n",
    "# Code to preprocess the tropical leaf-wood data of Louise Terryn\n",
    "#\n",
    "# The manually labeled wood points did not match the original point cloud due to precision mismatch when saving the files.\n",
    "# I thus saved the original point clouds with the same precision (%.3f). Then, I attributed a label to each point in the \n",
    "# original point cloud indicating whether the point is 'wood' or 'non-wood'. \n",
    "# \n",
    "# ---------------\n",
    "\n",
    "PATH_TREE = os.path.join(DATA_DIR, 'tree_points')\n",
    "PATH_WOOD = os.path.join(DATA_DIR, 'wood_points')\n",
    "plot_names = ['DRO', 'OC', 'RC']\n",
    "\n",
    "# Create different variables holding the filenames and paths\n",
    "filenames = {plot_name: os.listdir(os.path.join(PATH_TREE, plot_name)) for plot_name in plot_names}\n",
    "\n",
    "filenames_all = []\n",
    "for plot_name in plot_names:\n",
    "    filenames_all = filenames_all + filenames[plot_name]\n",
    "\n",
    "filepaths_tree = []\n",
    "for plot_name in plot_names:\n",
    "    filepaths_tree = filepaths_tree + [os.path.join(PATH_TREE, plot_name, filename) for filename in filenames[plot_name]]\n",
    "\n",
    "filepaths_wood = []\n",
    "for plot_name in plot_names:\n",
    "    filepaths_wood = filepaths_wood + [os.path.join(PATH_WOOD, plot_name, filename[:-6] + 'tls_0.02_wood.txt') for filename in filenames[plot_name]]\n",
    "\n",
    "\n",
    "def decrease_precision():\n",
    "\n",
    "    for file_in, filename_out in zip(filepaths_tree, filenames_all):\n",
    "        # Read tree\n",
    "        tree = np.loadtxt(file_in)   \n",
    "\n",
    "        if not os.path.exists(os.path.join(DATA_DIR, 'tmp')):\n",
    "                    os.makedirs(os.path.join(DATA_DIR, 'tmp'))\n",
    "\n",
    "        # Write file with lower precision\n",
    "        path_out = os.path.join(DATA_DIR, 'tmp', filename_out)\n",
    "        np.savetxt(path_out, tree, fmt='%.3f')\n",
    "\n",
    "\n",
    "def view1D(a, b): # a, b are arrays\n",
    "    a = np.ascontiguousarray(a)\n",
    "    b = np.ascontiguousarray(b)\n",
    "    void_dt = np.dtype((np.void, a.dtype.itemsize * a.shape[1]))\n",
    "    return a.view(void_dt).ravel(),  b.view(void_dt).ravel()\n",
    "\n",
    "\n",
    "def isin_nd(a,b):\n",
    "    # a,b are the 3D input arrays to give us \"isin-like\" functionality across them\n",
    "    A,B = view1D(a.reshape(a.shape[0],-1),b.reshape(b.shape[0],-1))\n",
    "    return np.isin(A,B)\n",
    "\n",
    "\n",
    "def add_label():\n",
    "\n",
    "     for filename, path_wood in zip(filenames_all, filepaths_wood):\n",
    "        # Read tree\n",
    "        tree = np.loadtxt(os.path.join(DATA_DIR, 'tmp', filename)) \n",
    "        wood = np.loadtxt(path_wood) \n",
    "        wood = wood[:, :3]\n",
    "\n",
    "        label = isin_nd(tree, wood)\n",
    "        file_out = np.hstack((tree, label.reshape(-1, 1)))\n",
    "     \n",
    "        if not os.path.exists(os.path.join(DATA_DIR, 'preprocessed')):\n",
    "            os.makedirs(os.path.join(DATA_DIR, 'preprocessed'))\n",
    "     \n",
    "        # Write file with label\n",
    "        path_out = os.path.join(DATA_DIR, 'preprocessed', filename)\n",
    "        np.savetxt(path_out, file_out, fmt='%.3f')\n",
    "\n",
    "\n",
    "# decrease_precision()\n",
    "# add_label()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Preprocess data to the file structure necessary for open3d ml datasets\n",
    "import random\n",
    "\n",
    "def convert_dataset():\n",
    "    DATA_PATH_IN = os.path.join(DATA_DIR, 'preprocessed') \n",
    "    DATA_PATH_OUT = os.path.join(DATA_DIR, 'preprocessed_open3d') \n",
    "\n",
    "    # Make required file structure\n",
    "    path_train = os.path.join(DATA_PATH_OUT, 'train')\n",
    "    path_val = os.path.join(DATA_PATH_OUT, 'val')\n",
    "    path_test = os.path.join(DATA_PATH_OUT, 'test')\n",
    "\n",
    "    for path in [path_train, path_val, path_test]:\n",
    "        if not os.path.exists(path):\n",
    "            os.makedirs(path)\n",
    "\n",
    "    # Get all filenames of point clouds\n",
    "    filenames = os.listdir(DATA_PATH_IN)\n",
    "\n",
    "    # Randomly shuffle the filenames\n",
    "    random.seed(42)\n",
    "    random.shuffle(filenames)\n",
    "\n",
    "    # Define train-val-test split\n",
    "    n_files = len(filenames)\n",
    "    split_train = 0.6\n",
    "    split_test = 0.2\n",
    "\n",
    "    # Get train-val-test files \n",
    "    files_train = filenames[:round(split_train*n_files)]\n",
    "    files_val = filenames[round(split_train*n_files):round((split_train + split_test)*n_files)]\n",
    "    files_test = filenames[round((split_train + split_test)*n_files):]\n",
    "\n",
    "\n",
    "    for files, path_out in zip([files_train, files_val], [path_train, path_val]):\n",
    "        for file in files:\n",
    "            # Read file\n",
    "            pcl = np.loadtxt(os.path.join(DATA_PATH_IN, file)) \n",
    "            # Write file\n",
    "            filename_out = os.path.join(path_out, file[:-3] + 'npy')\n",
    "            with open(filename_out, 'wb') as f:\n",
    "                np.save(f, pcl)\n",
    "\n",
    "    for file in files_test:\n",
    "        # Read file\n",
    "        pcl = np.loadtxt(os.path.join(DATA_PATH_IN, file)) \n",
    "\n",
    "        # Only retain xyz\n",
    "        xyz = pcl[:, :3]\n",
    "        labels = pcl[:, 3].astype(np.uint8)\n",
    "        \n",
    "        # Write file\n",
    "        filename_out = os.path.join(path_test, file[:-3] + 'npy')\n",
    "        with open(filename_out, 'wb') as f:\n",
    "            np.save(f, xyz)\n",
    "\n",
    "        filename_labels = os.path.join(path_test, file[:-4] + '_labels.txt')\n",
    "        np.savetxt(filename_labels, labels, fmt='%1.i')\n",
    "\n",
    "convert_dataset()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of trees: 60\n",
      "total number of points: 14364895\n",
      "average number of points per tree: 239414.91666666666\n",
      "max number of points: 1887667\n",
      "min number of points: 25483\n",
      "total number of woody points: 0\n",
      "total_number of non-wood points: 14364895\n",
      "fraction: 0.0\n"
     ]
    }
   ],
   "source": [
    "DATA_DIR = \"/mnt/c/Users/wavdnbro/OneDrive - UGent/Documents/spacetwin/datasets/leaf_wood/\"\n",
    "DATA_PATH = DATA_DIR + 'preprocessed_open3d/test/'\n",
    "\n",
    "n_trees = len(os.listdir(DATA_PATH))\n",
    "print('number of trees:', n_trees)\n",
    "\n",
    "total_points = 0\n",
    "wood_points = 0\n",
    "max_points = 0\n",
    "min_points = 100000000\n",
    "\n",
    "for filename in os.listdir(DATA_PATH):\n",
    "    # tree = np.load(DATA_PATH + filename)\n",
    "    if filename[-3:] == 'txt':\n",
    "        tree = np.loadtxt(DATA_PATH + filename) \n",
    "        # print(tree)\n",
    "    \n",
    "    total_points += len(tree)\n",
    "    # wood_points += tree[:, 3].sum()\n",
    "    # wood_points += tree.sum()\n",
    "    max_points = max(max_points, len(tree))\n",
    "    min_points = min(min_points, len(tree))\n",
    "\n",
    "\n",
    "print('total number of points:', total_points)\n",
    "print('average number of points per tree:', total_points / n_trees)\n",
    "print('max number of points:', max_points)\n",
    "print('min number of points:', min_points)\n",
    "print('total number of woody points:', wood_points)\n",
    "print('total_number of non-wood points:', total_points - wood_points)\n",
    "print('fraction:', wood_points / total_points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of species: 41\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv(\"/mnt/c/Users/wavdnbro/OneDrive - UGent/Documents/spacetwin/datasets/leaf_wood/AUS_samenvatting.csv\")\n",
    "\n",
    "print('number of species:', len(df.Species.unique()))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3D deep learning for semantic segmentation"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load config file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load config file\n",
    "cfg_path = \"cfg/randlanet_leafwood.yml\"\n",
    "cfg = _ml3d.utils.Config.load_from_file(cfg_path)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load dataset"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Use open3d provided dataset class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'point': array([[ -50.531, -246.818,  777.938],\n",
       "        [ -52.571, -246.596,  781.946],\n",
       "        [ -52.56 , -246.613,  781.906],\n",
       "        ...,\n",
       "        [ -49.49 , -240.964,  784.324],\n",
       "        [ -49.502, -240.977,  784.357],\n",
       "        [ -49.562, -240.332,  784.327]], dtype=float32),\n",
       " 'feat': None,\n",
       " 'label': array([0, 0, 0, ..., 0, 0, 0], dtype=int32)}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from open3d.ml.torch.datasets import Custom3D\n",
    "\n",
    "dataset = Custom3D(**cfg.dataset)\n",
    "\n",
    "dataset_train = dataset.get_split('train')\n",
    "dataset_train.get_data(0)\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Use custom Dataset class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "\n",
    "class MyDataset(torch.utils.data.Dataset):\n",
    "    \"\"\"\"\n",
    "    Custom Torch Dataset class to load the point cloud dataset for semantic segmenation into memory\n",
    "\n",
    "    Attributes:\n",
    "        data (list): list of  \n",
    "    \"\"\"\n",
    "    def __init__(self, dataset_path):\n",
    "        self.data = self._load_dataset(dataset_path)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        return self.data[index]\n",
    "    \n",
    "    def _load_dataset(self, dataset_path):\n",
    "        # Get all filenames of the point clouds (stored as .txt)\n",
    "        pcd_names = glob.glob(dataset_path + '/*.txt' )\n",
    "\n",
    "        data = []\n",
    "        for i in tqdm(range(len(pcd_names)), desc='loading dataset'):\n",
    "            # Load point cloud\n",
    "            pcd = np.loadtxt(os.path.join(dataset_path, pcd_names[i]))\n",
    "\n",
    "            # Append each point cloud as a list\n",
    "            data.append({\n",
    "                'point': pcd[:, :3], # xyz coordinates\n",
    "                'feat': None,        # the points do not have features\n",
    "                'label': pcd[:, 3].astype(np.int32),  # point wise label\n",
    "            })\n",
    "        \n",
    "        return data\n",
    "    \n",
    "\n",
    "DATA_PATH = \"/mnt/c/Users/wavdnbro/OneDrive - UGent/Documents/spacetwin/datasets/leaf_wood/preprocessed/\"\n",
    "dataset = MyDataset(DATA_PATH)\n",
    "\n",
    "\n",
    "\n",
    "# torch_data_train = mydataset(data_train)\n",
    "# dataload_train = DataLoader(torch_data_train, batch_size=batch_size, shuffle=True, num_workers=2)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Explore model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate model and pipeline\n",
    "model = ml3d.models.RandLANet(**cfg.model)\n",
    "pipeline = ml3d.pipelines.SemanticSegmentation(model, **cfg.pipeline)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([65536, 16])"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs['neighbor_indices'][0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(181570, 3)\n",
      "(178366, 3)\n",
      "(65536, 3)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_7653/1842002590.py:54: DeprecationWarning: NumPy will stop allowing conversion of out-of-bound Python integers to integer arrays.  The conversion of 5661592876527845271 to uint32 will fail in the future.\n",
      "For the old behavior, usually:\n",
      "    np.array(value).astype(dtype)`\n",
      "will give the desired result (the cast overflows).\n",
      "  worker_init_fn=lambda x: np.random.seed(x + np.uint32(torch.utils.data.get_worker_info().seed))\n",
      "/tmp/ipykernel_7653/1842002590.py:54: DeprecationWarning: NumPy will stop allowing conversion of out-of-bound Python integers to integer arrays.  The conversion of 5661592876527845272 to uint32 will fail in the future.\n",
      "For the old behavior, usually:\n",
      "    np.array(value).astype(dtype)`\n",
      "will give the desired result (the cast overflows).\n",
      "  worker_init_fn=lambda x: np.random.seed(x + np.uint32(torch.utils.data.get_worker_info().seed))\n",
      "/home/woutervdb/.local/lib/python3.10/site-packages/open3d/_ml3d/torch/dataloaders/default_batcher.py:50: UserWarning: An output with one or more elements was resized since it had shape [393216], which does not match the required output shape [2, 65536, 3]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:17.)\n",
      "  return torch.stack(batch, 0, out=out)\n",
      "/home/woutervdb/.local/lib/python3.10/site-packages/open3d/_ml3d/torch/dataloaders/default_batcher.py:50: UserWarning: An output with one or more elements was resized since it had shape [98304], which does not match the required output shape [2, 16384, 3]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:17.)\n",
      "  return torch.stack(batch, 0, out=out)\n",
      "/home/woutervdb/.local/lib/python3.10/site-packages/open3d/_ml3d/torch/dataloaders/default_batcher.py:50: UserWarning: An output with one or more elements was resized since it had shape [24576], which does not match the required output shape [2, 4096, 3]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:17.)\n",
      "  return torch.stack(batch, 0, out=out)\n",
      "/home/woutervdb/.local/lib/python3.10/site-packages/open3d/_ml3d/torch/dataloaders/default_batcher.py:50: UserWarning: An output with one or more elements was resized since it had shape [6144], which does not match the required output shape [2, 1024, 3]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:17.)\n",
      "  return torch.stack(batch, 0, out=out)\n",
      "/home/woutervdb/.local/lib/python3.10/site-packages/open3d/_ml3d/torch/dataloaders/default_batcher.py:50: UserWarning: An output with one or more elements was resized since it had shape [1536], which does not match the required output shape [2, 256, 3]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:17.)\n",
      "  return torch.stack(batch, 0, out=out)\n",
      "/home/woutervdb/.local/lib/python3.10/site-packages/open3d/_ml3d/torch/dataloaders/default_batcher.py:50: UserWarning: An output with one or more elements was resized since it had shape [2097152], which does not match the required output shape [2, 65536, 16]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:17.)\n",
      "  return torch.stack(batch, 0, out=out)\n",
      "/home/woutervdb/.local/lib/python3.10/site-packages/open3d/_ml3d/torch/dataloaders/default_batcher.py:50: UserWarning: An output with one or more elements was resized since it had shape [524288], which does not match the required output shape [2, 16384, 16]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:17.)\n",
      "  return torch.stack(batch, 0, out=out)\n",
      "/home/woutervdb/.local/lib/python3.10/site-packages/open3d/_ml3d/torch/dataloaders/default_batcher.py:50: UserWarning: An output with one or more elements was resized since it had shape [131072], which does not match the required output shape [2, 4096, 16]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:17.)\n",
      "  return torch.stack(batch, 0, out=out)\n",
      "/home/woutervdb/.local/lib/python3.10/site-packages/open3d/_ml3d/torch/dataloaders/default_batcher.py:50: UserWarning: An output with one or more elements was resized since it had shape [32768], which does not match the required output shape [2, 1024, 16]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:17.)\n",
      "  return torch.stack(batch, 0, out=out)\n",
      "/home/woutervdb/.local/lib/python3.10/site-packages/open3d/_ml3d/torch/dataloaders/default_batcher.py:50: UserWarning: An output with one or more elements was resized since it had shape [8192], which does not match the required output shape [2, 256, 16]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:17.)\n",
      "  return torch.stack(batch, 0, out=out)\n",
      "/home/woutervdb/.local/lib/python3.10/site-packages/open3d/_ml3d/torch/dataloaders/default_batcher.py:50: UserWarning: An output with one or more elements was resized since it had shape [4096], which does not match the required output shape [2, 128, 16]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:17.)\n",
      "  return torch.stack(batch, 0, out=out)\n",
      "/home/woutervdb/.local/lib/python3.10/site-packages/open3d/_ml3d/torch/dataloaders/default_batcher.py:50: UserWarning: An output with one or more elements was resized since it had shape [131072], which does not match the required output shape [2, 65536, 1]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:17.)\n",
      "  return torch.stack(batch, 0, out=out)\n",
      "/home/woutervdb/.local/lib/python3.10/site-packages/open3d/_ml3d/torch/dataloaders/default_batcher.py:50: UserWarning: An output with one or more elements was resized since it had shape [32768], which does not match the required output shape [2, 16384, 1]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:17.)\n",
      "  return torch.stack(batch, 0, out=out)\n",
      "/home/woutervdb/.local/lib/python3.10/site-packages/open3d/_ml3d/torch/dataloaders/default_batcher.py:50: UserWarning: An output with one or more elements was resized since it had shape [8192], which does not match the required output shape [2, 4096, 1]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:17.)\n",
      "  return torch.stack(batch, 0, out=out)\n",
      "/home/woutervdb/.local/lib/python3.10/site-packages/open3d/_ml3d/torch/dataloaders/default_batcher.py:50: UserWarning: An output with one or more elements was resized since it had shape [2048], which does not match the required output shape [2, 1024, 1]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:17.)\n",
      "  return torch.stack(batch, 0, out=out)\n",
      "/home/woutervdb/.local/lib/python3.10/site-packages/open3d/_ml3d/torch/dataloaders/default_batcher.py:50: UserWarning: An output with one or more elements was resized since it had shape [512], which does not match the required output shape [2, 256, 1]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:17.)\n",
      "  return torch.stack(batch, 0, out=out)\n",
      "/home/woutervdb/.local/lib/python3.10/site-packages/open3d/_ml3d/torch/dataloaders/default_batcher.py:50: UserWarning: An output with one or more elements was resized since it had shape [131072], which does not match the required output shape [2, 65536]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:17.)\n",
      "  return torch.stack(batch, 0, out=out)\n",
      "/home/woutervdb/.local/lib/python3.10/site-packages/open3d/_ml3d/torch/dataloaders/default_batcher.py:50: UserWarning: An output with one or more elements was resized since it had shape [393216], which does not match the required output shape [2, 65536, 3]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:17.)\n",
      "  return torch.stack(batch, 0, out=out)\n",
      "/home/woutervdb/.local/lib/python3.10/site-packages/open3d/_ml3d/torch/dataloaders/default_batcher.py:50: UserWarning: An output with one or more elements was resized since it had shape [98304], which does not match the required output shape [2, 16384, 3]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:17.)\n",
      "  return torch.stack(batch, 0, out=out)\n",
      "/home/woutervdb/.local/lib/python3.10/site-packages/open3d/_ml3d/torch/dataloaders/default_batcher.py:50: UserWarning: An output with one or more elements was resized since it had shape [24576], which does not match the required output shape [2, 4096, 3]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:17.)\n",
      "  return torch.stack(batch, 0, out=out)\n",
      "/home/woutervdb/.local/lib/python3.10/site-packages/open3d/_ml3d/torch/dataloaders/default_batcher.py:50: UserWarning: An output with one or more elements was resized since it had shape [6144], which does not match the required output shape [2, 1024, 3]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:17.)\n",
      "  return torch.stack(batch, 0, out=out)\n",
      "/home/woutervdb/.local/lib/python3.10/site-packages/open3d/_ml3d/torch/dataloaders/default_batcher.py:50: UserWarning: An output with one or more elements was resized since it had shape [1536], which does not match the required output shape [2, 256, 3]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:17.)\n",
      "  return torch.stack(batch, 0, out=out)\n",
      "/home/woutervdb/.local/lib/python3.10/site-packages/open3d/_ml3d/torch/dataloaders/default_batcher.py:50: UserWarning: An output with one or more elements was resized since it had shape [2097152], which does not match the required output shape [2, 65536, 16]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:17.)\n",
      "  return torch.stack(batch, 0, out=out)\n",
      "/home/woutervdb/.local/lib/python3.10/site-packages/open3d/_ml3d/torch/dataloaders/default_batcher.py:50: UserWarning: An output with one or more elements was resized since it had shape [524288], which does not match the required output shape [2, 16384, 16]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:17.)\n",
      "  return torch.stack(batch, 0, out=out)\n",
      "/home/woutervdb/.local/lib/python3.10/site-packages/open3d/_ml3d/torch/dataloaders/default_batcher.py:50: UserWarning: An output with one or more elements was resized since it had shape [131072], which does not match the required output shape [2, 4096, 16]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:17.)\n",
      "  return torch.stack(batch, 0, out=out)\n",
      "/home/woutervdb/.local/lib/python3.10/site-packages/open3d/_ml3d/torch/dataloaders/default_batcher.py:50: UserWarning: An output with one or more elements was resized since it had shape [32768], which does not match the required output shape [2, 1024, 16]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:17.)\n",
      "  return torch.stack(batch, 0, out=out)\n",
      "/home/woutervdb/.local/lib/python3.10/site-packages/open3d/_ml3d/torch/dataloaders/default_batcher.py:50: UserWarning: An output with one or more elements was resized since it had shape [8192], which does not match the required output shape [2, 256, 16]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:17.)\n",
      "  return torch.stack(batch, 0, out=out)\n",
      "/home/woutervdb/.local/lib/python3.10/site-packages/open3d/_ml3d/torch/dataloaders/default_batcher.py:50: UserWarning: An output with one or more elements was resized since it had shape [4096], which does not match the required output shape [2, 128, 16]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:17.)\n",
      "  return torch.stack(batch, 0, out=out)\n",
      "/home/woutervdb/.local/lib/python3.10/site-packages/open3d/_ml3d/torch/dataloaders/default_batcher.py:50: UserWarning: An output with one or more elements was resized since it had shape [131072], which does not match the required output shape [2, 65536, 1]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:17.)\n",
      "  return torch.stack(batch, 0, out=out)\n",
      "/home/woutervdb/.local/lib/python3.10/site-packages/open3d/_ml3d/torch/dataloaders/default_batcher.py:50: UserWarning: An output with one or more elements was resized since it had shape [32768], which does not match the required output shape [2, 16384, 1]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:17.)\n",
      "  return torch.stack(batch, 0, out=out)\n",
      "/home/woutervdb/.local/lib/python3.10/site-packages/open3d/_ml3d/torch/dataloaders/default_batcher.py:50: UserWarning: An output with one or more elements was resized since it had shape [8192], which does not match the required output shape [2, 4096, 1]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:17.)\n",
      "  return torch.stack(batch, 0, out=out)\n",
      "/home/woutervdb/.local/lib/python3.10/site-packages/open3d/_ml3d/torch/dataloaders/default_batcher.py:50: UserWarning: An output with one or more elements was resized since it had shape [2048], which does not match the required output shape [2, 1024, 1]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:17.)\n",
      "  return torch.stack(batch, 0, out=out)\n",
      "/home/woutervdb/.local/lib/python3.10/site-packages/open3d/_ml3d/torch/dataloaders/default_batcher.py:50: UserWarning: An output with one or more elements was resized since it had shape [512], which does not match the required output shape [2, 256, 1]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:17.)\n",
      "  return torch.stack(batch, 0, out=out)\n",
      "/home/woutervdb/.local/lib/python3.10/site-packages/open3d/_ml3d/torch/dataloaders/default_batcher.py:50: UserWarning: An output with one or more elements was resized since it had shape [131072], which does not match the required output shape [2, 65536]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:17.)\n",
      "  return torch.stack(batch, 0, out=out)\n",
      "/home/woutervdb/.local/lib/python3.10/site-packages/open3d/_ml3d/torch/dataloaders/default_batcher.py:50: UserWarning: An output with one or more elements was resized since it had shape [393216], which does not match the required output shape [2, 65536, 3]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:17.)\n",
      "  return torch.stack(batch, 0, out=out)\n",
      "/home/woutervdb/.local/lib/python3.10/site-packages/open3d/_ml3d/torch/dataloaders/default_batcher.py:50: UserWarning: An output with one or more elements was resized since it had shape [98304], which does not match the required output shape [2, 16384, 3]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:17.)\n",
      "  return torch.stack(batch, 0, out=out)\n",
      "/home/woutervdb/.local/lib/python3.10/site-packages/open3d/_ml3d/torch/dataloaders/default_batcher.py:50: UserWarning: An output with one or more elements was resized since it had shape [24576], which does not match the required output shape [2, 4096, 3]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:17.)\n",
      "  return torch.stack(batch, 0, out=out)\n",
      "/home/woutervdb/.local/lib/python3.10/site-packages/open3d/_ml3d/torch/dataloaders/default_batcher.py:50: UserWarning: An output with one or more elements was resized since it had shape [6144], which does not match the required output shape [2, 1024, 3]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:17.)\n",
      "  return torch.stack(batch, 0, out=out)\n",
      "/home/woutervdb/.local/lib/python3.10/site-packages/open3d/_ml3d/torch/dataloaders/default_batcher.py:50: UserWarning: An output with one or more elements was resized since it had shape [1536], which does not match the required output shape [2, 256, 3]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:17.)\n",
      "  return torch.stack(batch, 0, out=out)\n",
      "/home/woutervdb/.local/lib/python3.10/site-packages/open3d/_ml3d/torch/dataloaders/default_batcher.py:50: UserWarning: An output with one or more elements was resized since it had shape [2097152], which does not match the required output shape [2, 65536, 16]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:17.)\n",
      "  return torch.stack(batch, 0, out=out)\n",
      "/home/woutervdb/.local/lib/python3.10/site-packages/open3d/_ml3d/torch/dataloaders/default_batcher.py:50: UserWarning: An output with one or more elements was resized since it had shape [524288], which does not match the required output shape [2, 16384, 16]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:17.)\n",
      "  return torch.stack(batch, 0, out=out)\n",
      "/home/woutervdb/.local/lib/python3.10/site-packages/open3d/_ml3d/torch/dataloaders/default_batcher.py:50: UserWarning: An output with one or more elements was resized since it had shape [131072], which does not match the required output shape [2, 4096, 16]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:17.)\n",
      "  return torch.stack(batch, 0, out=out)\n",
      "/home/woutervdb/.local/lib/python3.10/site-packages/open3d/_ml3d/torch/dataloaders/default_batcher.py:50: UserWarning: An output with one or more elements was resized since it had shape [32768], which does not match the required output shape [2, 1024, 16]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:17.)\n",
      "  return torch.stack(batch, 0, out=out)\n",
      "/home/woutervdb/.local/lib/python3.10/site-packages/open3d/_ml3d/torch/dataloaders/default_batcher.py:50: UserWarning: An output with one or more elements was resized since it had shape [8192], which does not match the required output shape [2, 256, 16]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:17.)\n",
      "  return torch.stack(batch, 0, out=out)\n",
      "/home/woutervdb/.local/lib/python3.10/site-packages/open3d/_ml3d/torch/dataloaders/default_batcher.py:50: UserWarning: An output with one or more elements was resized since it had shape [4096], which does not match the required output shape [2, 128, 16]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:17.)\n",
      "  return torch.stack(batch, 0, out=out)\n",
      "/home/woutervdb/.local/lib/python3.10/site-packages/open3d/_ml3d/torch/dataloaders/default_batcher.py:50: UserWarning: An output with one or more elements was resized since it had shape [131072], which does not match the required output shape [2, 65536, 1]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:17.)\n",
      "  return torch.stack(batch, 0, out=out)\n",
      "/home/woutervdb/.local/lib/python3.10/site-packages/open3d/_ml3d/torch/dataloaders/default_batcher.py:50: UserWarning: An output with one or more elements was resized since it had shape [32768], which does not match the required output shape [2, 16384, 1]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:17.)\n",
      "  return torch.stack(batch, 0, out=out)\n",
      "/home/woutervdb/.local/lib/python3.10/site-packages/open3d/_ml3d/torch/dataloaders/default_batcher.py:50: UserWarning: An output with one or more elements was resized since it had shape [8192], which does not match the required output shape [2, 4096, 1]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:17.)\n",
      "  return torch.stack(batch, 0, out=out)\n",
      "/home/woutervdb/.local/lib/python3.10/site-packages/open3d/_ml3d/torch/dataloaders/default_batcher.py:50: UserWarning: An output with one or more elements was resized since it had shape [2048], which does not match the required output shape [2, 1024, 1]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:17.)\n",
      "  return torch.stack(batch, 0, out=out)\n",
      "/home/woutervdb/.local/lib/python3.10/site-packages/open3d/_ml3d/torch/dataloaders/default_batcher.py:50: UserWarning: An output with one or more elements was resized since it had shape [512], which does not match the required output shape [2, 256, 1]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:17.)\n",
      "  return torch.stack(batch, 0, out=out)\n",
      "/home/woutervdb/.local/lib/python3.10/site-packages/open3d/_ml3d/torch/dataloaders/default_batcher.py:50: UserWarning: An output with one or more elements was resized since it had shape [131072], which does not match the required output shape [2, 65536]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:17.)\n",
      "  return torch.stack(batch, 0, out=out)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[[ 0.0173, -0.7193],\n",
       "         [-0.0032, -0.5703],\n",
       "         [ 0.8324, -0.1347],\n",
       "         ...,\n",
       "         [-0.8508, -0.8310],\n",
       "         [ 0.5967, -0.7613],\n",
       "         [ 0.4142,  0.3307]],\n",
       "\n",
       "        [[-2.4192, -1.6776],\n",
       "         [-0.6914, -0.3464],\n",
       "         [-0.1225,  0.2538],\n",
       "         ...,\n",
       "         [-1.2953, -0.0731],\n",
       "         [ 0.5614, -1.2235],\n",
       "         [ 0.0233, -0.5581]]], grad_fn=<TransposeBackward0>)"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "attr = {'split':'train'}\n",
    "idx = 0\n",
    "input_pcl = dataset_train.get_data(idx)\n",
    "\n",
    "data = model.preprocess(input_pcl, attr)\n",
    "inputs = model.transform(data, attr)\n",
    "\n",
    "print(input_pcl['point'].shape)\n",
    "print(data['point'].shape)\n",
    "print(inputs['coords'][0].shape)\n",
    "\n",
    "# Save preprocessed and transformed point cloud with label\n",
    "# --> visualise in cloud compare to check. \n",
    "# input has been augmented\n",
    "pcl_transformed = np.hstack((inputs['coords'][0], inputs['labels'].reshape(-1, 1)))\n",
    "# np.savetxt('./tests/pcl_transformed_input_randlanet.txt', pcl_transformed, fmt='%.3f')\n",
    "\n",
    "# model.device = 'cpu'\n",
    "# inputs['coords'] = [torch.tensor(arr) for arr in inputs['coords']]\n",
    "# inputs['features'] = torch.tensor(np.expand_dims(inputs['features'], 0))\n",
    "# inputs['neighbor_indices'] = [torch.tensor(arr) for arr in inputs['neighbor_indices']]\n",
    "# inputs['sub_idx'] = [torch.tensor(arr) for arr in inputs['sub_idx']]\n",
    "# inputs['interp_idx'] = [torch.tensor(arr) for arr in inputs['interp_idx']]\n",
    "# inputs['point_inds'] = [torch.tensor(arr) for arr in inputs['point_inds']]\n",
    "# input1 = torch.tensor(np.expand_dims(inputs['coords'][0], 0).shape)\n",
    "\n",
    "# results = model.forward(inputs)\n",
    "# results\n",
    "\n",
    "train_dataset = dataset.get_split('train')\n",
    "train_sampler = train_dataset.sampler\n",
    "train_split = ml3d.dataloaders.TorchDataloader(\n",
    "    dataset=train_dataset,\n",
    "    preprocess=model.preprocess,\n",
    "    transform=model.transform,\n",
    "    sampler=train_sampler,\n",
    "    use_cache=dataset.cfg.use_cache,\n",
    "    steps_per_epoch=dataset.cfg.get('steps_per_epoch_train', None)\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "batcher = ml3d.dataloaders.DefaultBatcher()\n",
    "\n",
    "train_loader = DataLoader(\n",
    "    train_split,\n",
    "    batch_size=2,\n",
    "    sampler=ml3d.dataloaders.get_sampler(train_sampler),\n",
    "    num_workers=cfg.get('num_workers', 2),\n",
    "    pin_memory=cfg.get('pin_memory', True),\n",
    "    collate_fn=batcher.collate_fn,\n",
    "    worker_init_fn=lambda x: np.random.seed(x + np.uint32(torch.utils.data.get_worker_info().seed))\n",
    ")\n",
    "\n",
    "# for i, inputs in enumerate(train_loader):\n",
    "#     print(inputs)\n",
    "\n",
    "#     if i == 1:\n",
    "#         break\n",
    "\n",
    "inputs = next(iter(train_loader))\n",
    "\n",
    "results = model(inputs['data'])\n",
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 65536])\n",
      "torch.Size([2, 65536, 2])\n",
      "torch.Size([131072, 2])\n",
      "torch.Size([131072])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([0, 0, 0,  ..., 0, 0, 0])"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(inputs['data']['labels'].shape)\n",
    "print(results.shape)\n",
    "\n",
    "from open3d.ml.torch.modules import losses\n",
    "scores, labels = losses.filter_valid_label(\n",
    "    results, \n",
    "    inputs['data']['labels'], \n",
    "    2,\n",
    "    [],\n",
    "    'cpu'\n",
    ")\n",
    "\n",
    "print(scores.shape)\n",
    "print(labels.shape)\n",
    "\n",
    "labels"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Instantiate model with custom loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'cfg' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 65\u001b[0m\n\u001b[1;32m     43\u001b[0m         \u001b[39mreturn\u001b[39;00m loss, labels, scores\n\u001b[1;32m     46\u001b[0m \u001b[39m# class CategoricalCrossEntropy:\u001b[39;00m\n\u001b[1;32m     47\u001b[0m \u001b[39m#     def __init__(self, num_per_class) -> None:\u001b[39;00m\n\u001b[1;32m     48\u001b[0m \u001b[39m#         self.softmax = torch.nn.Softmax(dim=-1)\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     61\u001b[0m \u001b[39m#         ce_label_weight = 1 / (weight + 0.02)\u001b[39;00m\n\u001b[1;32m     62\u001b[0m \u001b[39m#         return torch.tensor(np.expand_dims(ce_label_weight, axis=0))\u001b[39;00m\n\u001b[0;32m---> 65\u001b[0m model \u001b[39m=\u001b[39m CustomRandLANet(\u001b[39m*\u001b[39m\u001b[39m*\u001b[39mcfg\u001b[39m.\u001b[39mmodel)\n\u001b[1;32m     66\u001b[0m model\u001b[39m.\u001b[39mclass_weights\n",
      "\u001b[0;31mNameError\u001b[0m: name 'cfg' is not defined"
     ]
    }
   ],
   "source": [
    "from open3d.ml.torch.modules import losses\n",
    "import torch.nn as nn\n",
    "\n",
    "# ml3d.modules.losses.filter_valid_label()\n",
    "\n",
    "class CustomRandLANet(ml3d.models.RandLANet):\n",
    "    # def __init__(self, name='RandLANet', num_neighbors=16, num_layers=4, num_points=4096 * 11, num_classes=19, ignored_label_inds=..., sub_sampling_ratio=..., in_channels=3, dim_features=8, dim_output=..., grid_size=0.06, batcher='DefaultBatcher', ckpt_path=None, augment=..., **kwargs):\n",
    "    #     super().__init__(name, num_neighbors, num_layers, num_points, num_classes, ignored_label_inds, sub_sampling_ratio, in_channels, dim_features, dim_output, grid_size, batcher, ckpt_path, augment, **kwargs)\n",
    "\n",
    "    #     # self.loss_fn = CategoricalCrossEntropy(cfg.dataset.get('class_weights'))\n",
    "    #     class_weights = self.get_class_weights(cfg.dataset.get('class_weights'))\n",
    "    #     self.cce_loss = nn.CrossEntropyLoss(weight=class_weights)\n",
    "\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "\n",
    "        num_per_class = cfg.dataset.get('class_weights', None)\n",
    "        self.class_weights = self._get_class_weights(num_per_class) if num_per_class is not None else None\n",
    "        self.cce_loss = nn.CrossEntropyLoss(weight=self.class_weights)\n",
    "\n",
    "    def _get_class_weights(self, num_per_class):\n",
    "        # pre-calculate the number of points in each category\n",
    "        num_per_class = np.array(num_per_class, dtype=np.float32)\n",
    "        weight = num_per_class / float(sum(num_per_class))\n",
    "        ce_label_weight = 1 / (weight + 0.02)\n",
    "        return torch.tensor(ce_label_weight)\n",
    "    \n",
    "    def get_loss(self, Loss, results, inputs, device):\n",
    "        labels = inputs['data']['labels']\n",
    "\n",
    "        scores, labels = losses.filter_valid_label(\n",
    "            results, \n",
    "            labels, \n",
    "            self.cfg.num_classes,\n",
    "            self.cfg.ignored_label_inds,\n",
    "            device\n",
    "        )\n",
    "\n",
    "        loss = self.cce_loss(scores, labels)\n",
    "        \n",
    "        # loss = Loss.weighted_CrossEntropyLoss(scores, labels)\n",
    "\n",
    "        return loss, labels, scores\n",
    "\n",
    "\n",
    "# class CategoricalCrossEntropy:\n",
    "#     def __init__(self, num_per_class) -> None:\n",
    "#         self.softmax = torch.nn.Softmax(dim=-1)\n",
    "#         class_weights = self.get_class_weights(num_per_class)\n",
    "#         self.cce = nn.CrossEntropyLoss(weight=class_weights)\n",
    "\n",
    "        \n",
    "#     def __call__(self, *args: Any, **kwds: Any) -> Any:\n",
    "#         pass\n",
    "        \n",
    "        \n",
    "#     def get_class_weights(num_per_class):\n",
    "#         # pre-calculate the number of points in each category\n",
    "#         num_per_class = np.array(num_per_class, dtype=np.float32)\n",
    "#         weight = num_per_class / float(sum(num_per_class))\n",
    "#         ce_label_weight = 1 / (weight + 0.02)\n",
    "#         return torch.tensor(np.expand_dims(ce_label_weight, axis=0))\n",
    "\n",
    "\n",
    "model = CustomRandLANet(**cfg.model)\n",
    "model.class_weights"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Explore loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1.233279  4.3638883]] [0.7908465  0.20915344]\n"
     ]
    }
   ],
   "source": [
    "def get_class_weights(num_per_class):\n",
    "        # pre-calculate the number of points in each category\n",
    "        num_per_class = np.array(num_per_class, dtype=np.float32)\n",
    "\n",
    "        weight = num_per_class / float(sum(num_per_class))\n",
    "        ce_label_weight = 1 / (weight + 0.02)\n",
    "\n",
    "        return np.expand_dims(ce_label_weight, axis=0), weight\n",
    "\n",
    "num_per_class = [34512040, 9127323]\n",
    "\n",
    "lossweight, weight = get_class_weights(num_per_class)\n",
    "print(lossweight, weight)\n",
    "\n",
    "# Loss = ml3d.modules.losses.SemSegLoss(pipeline, model, dataset, device='cpu')\n",
    "# model.get_loss(Loss, results, inputs, device='cpu')\n",
    "\n",
    "# TODO: training gives error when providing class weights. Problem is with shape that get_class_weights returns"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training:   0%|          | 0/45 [00:00<?, ?it/s]/home/woutervdb/.local/lib/python3.10/site-packages/open3d/_ml3d/torch/dataloaders/default_batcher.py:50: UserWarning: An output with one or more elements was resized since it had shape [393216], which does not match the required output shape [2, 65536, 3]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:17.)\n",
      "  return torch.stack(batch, 0, out=out)\n",
      "/home/woutervdb/.local/lib/python3.10/site-packages/open3d/_ml3d/torch/dataloaders/default_batcher.py:50: UserWarning: An output with one or more elements was resized since it had shape [98304], which does not match the required output shape [2, 16384, 3]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:17.)\n",
      "  return torch.stack(batch, 0, out=out)\n",
      "/home/woutervdb/.local/lib/python3.10/site-packages/open3d/_ml3d/torch/dataloaders/default_batcher.py:50: UserWarning: An output with one or more elements was resized since it had shape [24576], which does not match the required output shape [2, 4096, 3]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:17.)\n",
      "  return torch.stack(batch, 0, out=out)\n",
      "/home/woutervdb/.local/lib/python3.10/site-packages/open3d/_ml3d/torch/dataloaders/default_batcher.py:50: UserWarning: An output with one or more elements was resized since it had shape [6144], which does not match the required output shape [2, 1024, 3]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:17.)\n",
      "  return torch.stack(batch, 0, out=out)\n",
      "/home/woutervdb/.local/lib/python3.10/site-packages/open3d/_ml3d/torch/dataloaders/default_batcher.py:50: UserWarning: An output with one or more elements was resized since it had shape [1536], which does not match the required output shape [2, 256, 3]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:17.)\n",
      "  return torch.stack(batch, 0, out=out)\n",
      "/home/woutervdb/.local/lib/python3.10/site-packages/open3d/_ml3d/torch/dataloaders/default_batcher.py:50: UserWarning: An output with one or more elements was resized since it had shape [393216], which does not match the required output shape [2, 65536, 3]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:17.)\n",
      "  return torch.stack(batch, 0, out=out)\n",
      "/home/woutervdb/.local/lib/python3.10/site-packages/open3d/_ml3d/torch/dataloaders/default_batcher.py:50: UserWarning: An output with one or more elements was resized since it had shape [98304], which does not match the required output shape [2, 16384, 3]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:17.)\n",
      "  return torch.stack(batch, 0, out=out)\n",
      "/home/woutervdb/.local/lib/python3.10/site-packages/open3d/_ml3d/torch/dataloaders/default_batcher.py:50: UserWarning: An output with one or more elements was resized since it had shape [24576], which does not match the required output shape [2, 4096, 3]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:17.)\n",
      "  return torch.stack(batch, 0, out=out)\n",
      "/home/woutervdb/.local/lib/python3.10/site-packages/open3d/_ml3d/torch/dataloaders/default_batcher.py:50: UserWarning: An output with one or more elements was resized since it had shape [6144], which does not match the required output shape [2, 1024, 3]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:17.)\n",
      "  return torch.stack(batch, 0, out=out)\n",
      "/home/woutervdb/.local/lib/python3.10/site-packages/open3d/_ml3d/torch/dataloaders/default_batcher.py:50: UserWarning: An output with one or more elements was resized since it had shape [1536], which does not match the required output shape [2, 256, 3]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:17.)\n",
      "  return torch.stack(batch, 0, out=out)\n",
      "/home/woutervdb/.local/lib/python3.10/site-packages/open3d/_ml3d/torch/dataloaders/default_batcher.py:50: UserWarning: An output with one or more elements was resized since it had shape [2097152], which does not match the required output shape [2, 65536, 16]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:17.)\n",
      "  return torch.stack(batch, 0, out=out)\n",
      "/home/woutervdb/.local/lib/python3.10/site-packages/open3d/_ml3d/torch/dataloaders/default_batcher.py:50: UserWarning: An output with one or more elements was resized since it had shape [524288], which does not match the required output shape [2, 16384, 16]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:17.)\n",
      "  return torch.stack(batch, 0, out=out)\n",
      "/home/woutervdb/.local/lib/python3.10/site-packages/open3d/_ml3d/torch/dataloaders/default_batcher.py:50: UserWarning: An output with one or more elements was resized since it had shape [2097152], which does not match the required output shape [2, 65536, 16]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:17.)\n",
      "  return torch.stack(batch, 0, out=out)\n",
      "/home/woutervdb/.local/lib/python3.10/site-packages/open3d/_ml3d/torch/dataloaders/default_batcher.py:50: UserWarning: An output with one or more elements was resized since it had shape [131072], which does not match the required output shape [2, 4096, 16]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:17.)\n",
      "  return torch.stack(batch, 0, out=out)\n",
      "/home/woutervdb/.local/lib/python3.10/site-packages/open3d/_ml3d/torch/dataloaders/default_batcher.py:50: UserWarning: An output with one or more elements was resized since it had shape [524288], which does not match the required output shape [2, 16384, 16]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:17.)\n",
      "  return torch.stack(batch, 0, out=out)\n",
      "/home/woutervdb/.local/lib/python3.10/site-packages/open3d/_ml3d/torch/dataloaders/default_batcher.py:50: UserWarning: An output with one or more elements was resized since it had shape [32768], which does not match the required output shape [2, 1024, 16]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:17.)\n",
      "  return torch.stack(batch, 0, out=out)\n",
      "/home/woutervdb/.local/lib/python3.10/site-packages/open3d/_ml3d/torch/dataloaders/default_batcher.py:50: UserWarning: An output with one or more elements was resized since it had shape [131072], which does not match the required output shape [2, 4096, 16]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:17.)\n",
      "  return torch.stack(batch, 0, out=out)\n",
      "/home/woutervdb/.local/lib/python3.10/site-packages/open3d/_ml3d/torch/dataloaders/default_batcher.py:50: UserWarning: An output with one or more elements was resized since it had shape [8192], which does not match the required output shape [2, 256, 16]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:17.)\n",
      "  return torch.stack(batch, 0, out=out)\n",
      "/home/woutervdb/.local/lib/python3.10/site-packages/open3d/_ml3d/torch/dataloaders/default_batcher.py:50: UserWarning: An output with one or more elements was resized since it had shape [32768], which does not match the required output shape [2, 1024, 16]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:17.)\n",
      "  return torch.stack(batch, 0, out=out)\n",
      "/home/woutervdb/.local/lib/python3.10/site-packages/open3d/_ml3d/torch/dataloaders/default_batcher.py:50: UserWarning: An output with one or more elements was resized since it had shape [8192], which does not match the required output shape [2, 256, 16]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:17.)\n",
      "  return torch.stack(batch, 0, out=out)\n",
      "/home/woutervdb/.local/lib/python3.10/site-packages/open3d/_ml3d/torch/dataloaders/default_batcher.py:50: UserWarning: An output with one or more elements was resized since it had shape [4096], which does not match the required output shape [2, 128, 16]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:17.)\n",
      "  return torch.stack(batch, 0, out=out)\n",
      "/home/woutervdb/.local/lib/python3.10/site-packages/open3d/_ml3d/torch/dataloaders/default_batcher.py:50: UserWarning: An output with one or more elements was resized since it had shape [4096], which does not match the required output shape [2, 128, 16]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:17.)\n",
      "  return torch.stack(batch, 0, out=out)\n",
      "/home/woutervdb/.local/lib/python3.10/site-packages/open3d/_ml3d/torch/dataloaders/default_batcher.py:50: UserWarning: An output with one or more elements was resized since it had shape [131072], which does not match the required output shape [2, 65536, 1]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:17.)\n",
      "  return torch.stack(batch, 0, out=out)\n",
      "/home/woutervdb/.local/lib/python3.10/site-packages/open3d/_ml3d/torch/dataloaders/default_batcher.py:50: UserWarning: An output with one or more elements was resized since it had shape [131072], which does not match the required output shape [2, 65536, 1]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:17.)\n",
      "  return torch.stack(batch, 0, out=out)\n",
      "/home/woutervdb/.local/lib/python3.10/site-packages/open3d/_ml3d/torch/dataloaders/default_batcher.py:50: UserWarning: An output with one or more elements was resized since it had shape [32768], which does not match the required output shape [2, 16384, 1]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:17.)\n",
      "  return torch.stack(batch, 0, out=out)\n",
      "/home/woutervdb/.local/lib/python3.10/site-packages/open3d/_ml3d/torch/dataloaders/default_batcher.py:50: UserWarning: An output with one or more elements was resized since it had shape [32768], which does not match the required output shape [2, 16384, 1]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:17.)\n",
      "  return torch.stack(batch, 0, out=out)\n",
      "/home/woutervdb/.local/lib/python3.10/site-packages/open3d/_ml3d/torch/dataloaders/default_batcher.py:50: UserWarning: An output with one or more elements was resized since it had shape [8192], which does not match the required output shape [2, 4096, 1]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:17.)\n",
      "  return torch.stack(batch, 0, out=out)\n",
      "/home/woutervdb/.local/lib/python3.10/site-packages/open3d/_ml3d/torch/dataloaders/default_batcher.py:50: UserWarning: An output with one or more elements was resized since it had shape [8192], which does not match the required output shape [2, 4096, 1]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:17.)\n",
      "  return torch.stack(batch, 0, out=out)\n",
      "/home/woutervdb/.local/lib/python3.10/site-packages/open3d/_ml3d/torch/dataloaders/default_batcher.py:50: UserWarning: An output with one or more elements was resized since it had shape [2048], which does not match the required output shape [2, 1024, 1]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:17.)\n",
      "  return torch.stack(batch, 0, out=out)\n",
      "/home/woutervdb/.local/lib/python3.10/site-packages/open3d/_ml3d/torch/dataloaders/default_batcher.py:50: UserWarning: An output with one or more elements was resized since it had shape [2048], which does not match the required output shape [2, 1024, 1]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:17.)\n",
      "  return torch.stack(batch, 0, out=out)\n",
      "/home/woutervdb/.local/lib/python3.10/site-packages/open3d/_ml3d/torch/dataloaders/default_batcher.py:50: UserWarning: An output with one or more elements was resized since it had shape [512], which does not match the required output shape [2, 256, 1]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:17.)\n",
      "  return torch.stack(batch, 0, out=out)\n",
      "/home/woutervdb/.local/lib/python3.10/site-packages/open3d/_ml3d/torch/dataloaders/default_batcher.py:50: UserWarning: An output with one or more elements was resized since it had shape [512], which does not match the required output shape [2, 256, 1]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:17.)\n",
      "  return torch.stack(batch, 0, out=out)\n",
      "/home/woutervdb/.local/lib/python3.10/site-packages/open3d/_ml3d/torch/dataloaders/default_batcher.py:50: UserWarning: An output with one or more elements was resized since it had shape [131072], which does not match the required output shape [2, 65536]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:17.)\n",
      "  return torch.stack(batch, 0, out=out)\n",
      "/home/woutervdb/.local/lib/python3.10/site-packages/open3d/_ml3d/torch/dataloaders/default_batcher.py:50: UserWarning: An output with one or more elements was resized since it had shape [131072], which does not match the required output shape [2, 65536]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:17.)\n",
      "  return torch.stack(batch, 0, out=out)\n",
      "/home/woutervdb/.local/lib/python3.10/site-packages/open3d/_ml3d/torch/dataloaders/default_batcher.py:50: UserWarning: An output with one or more elements was resized since it had shape [393216], which does not match the required output shape [2, 65536, 3]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:17.)\n",
      "  return torch.stack(batch, 0, out=out)\n",
      "/home/woutervdb/.local/lib/python3.10/site-packages/open3d/_ml3d/torch/dataloaders/default_batcher.py:50: UserWarning: An output with one or more elements was resized since it had shape [98304], which does not match the required output shape [2, 16384, 3]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:17.)\n",
      "  return torch.stack(batch, 0, out=out)\n",
      "/home/woutervdb/.local/lib/python3.10/site-packages/open3d/_ml3d/torch/dataloaders/default_batcher.py:50: UserWarning: An output with one or more elements was resized since it had shape [24576], which does not match the required output shape [2, 4096, 3]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:17.)\n",
      "  return torch.stack(batch, 0, out=out)\n",
      "/home/woutervdb/.local/lib/python3.10/site-packages/open3d/_ml3d/torch/dataloaders/default_batcher.py:50: UserWarning: An output with one or more elements was resized since it had shape [6144], which does not match the required output shape [2, 1024, 3]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:17.)\n",
      "  return torch.stack(batch, 0, out=out)\n",
      "/home/woutervdb/.local/lib/python3.10/site-packages/open3d/_ml3d/torch/dataloaders/default_batcher.py:50: UserWarning: An output with one or more elements was resized since it had shape [1536], which does not match the required output shape [2, 256, 3]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:17.)\n",
      "  return torch.stack(batch, 0, out=out)\n",
      "/home/woutervdb/.local/lib/python3.10/site-packages/open3d/_ml3d/torch/dataloaders/default_batcher.py:50: UserWarning: An output with one or more elements was resized since it had shape [2097152], which does not match the required output shape [2, 65536, 16]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:17.)\n",
      "  return torch.stack(batch, 0, out=out)\n",
      "/home/woutervdb/.local/lib/python3.10/site-packages/open3d/_ml3d/torch/dataloaders/default_batcher.py:50: UserWarning: An output with one or more elements was resized since it had shape [524288], which does not match the required output shape [2, 16384, 16]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:17.)\n",
      "  return torch.stack(batch, 0, out=out)\n",
      "/home/woutervdb/.local/lib/python3.10/site-packages/open3d/_ml3d/torch/dataloaders/default_batcher.py:50: UserWarning: An output with one or more elements was resized since it had shape [131072], which does not match the required output shape [2, 4096, 16]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:17.)\n",
      "  return torch.stack(batch, 0, out=out)\n",
      "/home/woutervdb/.local/lib/python3.10/site-packages/open3d/_ml3d/torch/dataloaders/default_batcher.py:50: UserWarning: An output with one or more elements was resized since it had shape [32768], which does not match the required output shape [2, 1024, 16]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:17.)\n",
      "  return torch.stack(batch, 0, out=out)\n",
      "/home/woutervdb/.local/lib/python3.10/site-packages/open3d/_ml3d/torch/dataloaders/default_batcher.py:50: UserWarning: An output with one or more elements was resized since it had shape [8192], which does not match the required output shape [2, 256, 16]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:17.)\n",
      "  return torch.stack(batch, 0, out=out)\n",
      "/home/woutervdb/.local/lib/python3.10/site-packages/open3d/_ml3d/torch/dataloaders/default_batcher.py:50: UserWarning: An output with one or more elements was resized since it had shape [4096], which does not match the required output shape [2, 128, 16]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:17.)\n",
      "  return torch.stack(batch, 0, out=out)\n",
      "/home/woutervdb/.local/lib/python3.10/site-packages/open3d/_ml3d/torch/dataloaders/default_batcher.py:50: UserWarning: An output with one or more elements was resized since it had shape [131072], which does not match the required output shape [2, 65536, 1]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:17.)\n",
      "  return torch.stack(batch, 0, out=out)\n",
      "/home/woutervdb/.local/lib/python3.10/site-packages/open3d/_ml3d/torch/dataloaders/default_batcher.py:50: UserWarning: An output with one or more elements was resized since it had shape [32768], which does not match the required output shape [2, 16384, 1]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:17.)\n",
      "  return torch.stack(batch, 0, out=out)\n",
      "/home/woutervdb/.local/lib/python3.10/site-packages/open3d/_ml3d/torch/dataloaders/default_batcher.py:50: UserWarning: An output with one or more elements was resized since it had shape [8192], which does not match the required output shape [2, 4096, 1]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:17.)\n",
      "  return torch.stack(batch, 0, out=out)\n",
      "/home/woutervdb/.local/lib/python3.10/site-packages/open3d/_ml3d/torch/dataloaders/default_batcher.py:50: UserWarning: An output with one or more elements was resized since it had shape [2048], which does not match the required output shape [2, 1024, 1]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:17.)\n",
      "  return torch.stack(batch, 0, out=out)\n",
      "/home/woutervdb/.local/lib/python3.10/site-packages/open3d/_ml3d/torch/dataloaders/default_batcher.py:50: UserWarning: An output with one or more elements was resized since it had shape [512], which does not match the required output shape [2, 256, 1]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:17.)\n",
      "  return torch.stack(batch, 0, out=out)\n",
      "/home/woutervdb/.local/lib/python3.10/site-packages/open3d/_ml3d/torch/dataloaders/default_batcher.py:50: UserWarning: An output with one or more elements was resized since it had shape [131072], which does not match the required output shape [2, 65536]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:17.)\n",
      "  return torch.stack(batch, 0, out=out)\n",
      "/home/woutervdb/.local/lib/python3.10/site-packages/open3d/_ml3d/torch/dataloaders/default_batcher.py:50: UserWarning: An output with one or more elements was resized since it had shape [393216], which does not match the required output shape [2, 65536, 3]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:17.)\n",
      "  return torch.stack(batch, 0, out=out)\n",
      "/home/woutervdb/.local/lib/python3.10/site-packages/open3d/_ml3d/torch/dataloaders/default_batcher.py:50: UserWarning: An output with one or more elements was resized since it had shape [98304], which does not match the required output shape [2, 16384, 3]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:17.)\n",
      "  return torch.stack(batch, 0, out=out)\n",
      "/home/woutervdb/.local/lib/python3.10/site-packages/open3d/_ml3d/torch/dataloaders/default_batcher.py:50: UserWarning: An output with one or more elements was resized since it had shape [24576], which does not match the required output shape [2, 4096, 3]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:17.)\n",
      "  return torch.stack(batch, 0, out=out)\n",
      "/home/woutervdb/.local/lib/python3.10/site-packages/open3d/_ml3d/torch/dataloaders/default_batcher.py:50: UserWarning: An output with one or more elements was resized since it had shape [6144], which does not match the required output shape [2, 1024, 3]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:17.)\n",
      "  return torch.stack(batch, 0, out=out)\n",
      "/home/woutervdb/.local/lib/python3.10/site-packages/open3d/_ml3d/torch/dataloaders/default_batcher.py:50: UserWarning: An output with one or more elements was resized since it had shape [1536], which does not match the required output shape [2, 256, 3]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:17.)\n",
      "  return torch.stack(batch, 0, out=out)\n",
      "/home/woutervdb/.local/lib/python3.10/site-packages/open3d/_ml3d/torch/dataloaders/default_batcher.py:50: UserWarning: An output with one or more elements was resized since it had shape [2097152], which does not match the required output shape [2, 65536, 16]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:17.)\n",
      "  return torch.stack(batch, 0, out=out)\n",
      "/home/woutervdb/.local/lib/python3.10/site-packages/open3d/_ml3d/torch/dataloaders/default_batcher.py:50: UserWarning: An output with one or more elements was resized since it had shape [524288], which does not match the required output shape [2, 16384, 16]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:17.)\n",
      "  return torch.stack(batch, 0, out=out)\n",
      "/home/woutervdb/.local/lib/python3.10/site-packages/open3d/_ml3d/torch/dataloaders/default_batcher.py:50: UserWarning: An output with one or more elements was resized since it had shape [131072], which does not match the required output shape [2, 4096, 16]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:17.)\n",
      "  return torch.stack(batch, 0, out=out)\n",
      "/home/woutervdb/.local/lib/python3.10/site-packages/open3d/_ml3d/torch/dataloaders/default_batcher.py:50: UserWarning: An output with one or more elements was resized since it had shape [32768], which does not match the required output shape [2, 1024, 16]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:17.)\n",
      "  return torch.stack(batch, 0, out=out)\n",
      "/home/woutervdb/.local/lib/python3.10/site-packages/open3d/_ml3d/torch/dataloaders/default_batcher.py:50: UserWarning: An output with one or more elements was resized since it had shape [8192], which does not match the required output shape [2, 256, 16]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:17.)\n",
      "  return torch.stack(batch, 0, out=out)\n",
      "/home/woutervdb/.local/lib/python3.10/site-packages/open3d/_ml3d/torch/dataloaders/default_batcher.py:50: UserWarning: An output with one or more elements was resized since it had shape [4096], which does not match the required output shape [2, 128, 16]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:17.)\n",
      "  return torch.stack(batch, 0, out=out)\n",
      "/home/woutervdb/.local/lib/python3.10/site-packages/open3d/_ml3d/torch/dataloaders/default_batcher.py:50: UserWarning: An output with one or more elements was resized since it had shape [131072], which does not match the required output shape [2, 65536, 1]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:17.)\n",
      "  return torch.stack(batch, 0, out=out)\n",
      "/home/woutervdb/.local/lib/python3.10/site-packages/open3d/_ml3d/torch/dataloaders/default_batcher.py:50: UserWarning: An output with one or more elements was resized since it had shape [32768], which does not match the required output shape [2, 16384, 1]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:17.)\n",
      "  return torch.stack(batch, 0, out=out)\n",
      "/home/woutervdb/.local/lib/python3.10/site-packages/open3d/_ml3d/torch/dataloaders/default_batcher.py:50: UserWarning: An output with one or more elements was resized since it had shape [8192], which does not match the required output shape [2, 4096, 1]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:17.)\n",
      "  return torch.stack(batch, 0, out=out)\n",
      "/home/woutervdb/.local/lib/python3.10/site-packages/open3d/_ml3d/torch/dataloaders/default_batcher.py:50: UserWarning: An output with one or more elements was resized since it had shape [2048], which does not match the required output shape [2, 1024, 1]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:17.)\n",
      "  return torch.stack(batch, 0, out=out)\n",
      "/home/woutervdb/.local/lib/python3.10/site-packages/open3d/_ml3d/torch/dataloaders/default_batcher.py:50: UserWarning: An output with one or more elements was resized since it had shape [512], which does not match the required output shape [2, 256, 1]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:17.)\n",
      "  return torch.stack(batch, 0, out=out)\n",
      "/home/woutervdb/.local/lib/python3.10/site-packages/open3d/_ml3d/torch/dataloaders/default_batcher.py:50: UserWarning: An output with one or more elements was resized since it had shape [131072], which does not match the required output shape [2, 65536]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:17.)\n",
      "  return torch.stack(batch, 0, out=out)\n",
      "/home/woutervdb/.local/lib/python3.10/site-packages/open3d/_ml3d/torch/dataloaders/default_batcher.py:50: UserWarning: An output with one or more elements was resized since it had shape [393216], which does not match the required output shape [2, 65536, 3]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:17.)\n",
      "  return torch.stack(batch, 0, out=out)\n",
      "/home/woutervdb/.local/lib/python3.10/site-packages/open3d/_ml3d/torch/dataloaders/default_batcher.py:50: UserWarning: An output with one or more elements was resized since it had shape [98304], which does not match the required output shape [2, 16384, 3]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:17.)\n",
      "  return torch.stack(batch, 0, out=out)\n",
      "/home/woutervdb/.local/lib/python3.10/site-packages/open3d/_ml3d/torch/dataloaders/default_batcher.py:50: UserWarning: An output with one or more elements was resized since it had shape [24576], which does not match the required output shape [2, 4096, 3]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:17.)\n",
      "  return torch.stack(batch, 0, out=out)\n",
      "/home/woutervdb/.local/lib/python3.10/site-packages/open3d/_ml3d/torch/dataloaders/default_batcher.py:50: UserWarning: An output with one or more elements was resized since it had shape [6144], which does not match the required output shape [2, 1024, 3]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:17.)\n",
      "  return torch.stack(batch, 0, out=out)\n",
      "/home/woutervdb/.local/lib/python3.10/site-packages/open3d/_ml3d/torch/dataloaders/default_batcher.py:50: UserWarning: An output with one or more elements was resized since it had shape [1536], which does not match the required output shape [2, 256, 3]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:17.)\n",
      "  return torch.stack(batch, 0, out=out)\n",
      "/home/woutervdb/.local/lib/python3.10/site-packages/open3d/_ml3d/torch/dataloaders/default_batcher.py:50: UserWarning: An output with one or more elements was resized since it had shape [2097152], which does not match the required output shape [2, 65536, 16]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:17.)\n",
      "  return torch.stack(batch, 0, out=out)\n",
      "/home/woutervdb/.local/lib/python3.10/site-packages/open3d/_ml3d/torch/dataloaders/default_batcher.py:50: UserWarning: An output with one or more elements was resized since it had shape [524288], which does not match the required output shape [2, 16384, 16]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:17.)\n",
      "  return torch.stack(batch, 0, out=out)\n",
      "/home/woutervdb/.local/lib/python3.10/site-packages/open3d/_ml3d/torch/dataloaders/default_batcher.py:50: UserWarning: An output with one or more elements was resized since it had shape [131072], which does not match the required output shape [2, 4096, 16]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:17.)\n",
      "  return torch.stack(batch, 0, out=out)\n",
      "/home/woutervdb/.local/lib/python3.10/site-packages/open3d/_ml3d/torch/dataloaders/default_batcher.py:50: UserWarning: An output with one or more elements was resized since it had shape [32768], which does not match the required output shape [2, 1024, 16]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:17.)\n",
      "  return torch.stack(batch, 0, out=out)\n",
      "/home/woutervdb/.local/lib/python3.10/site-packages/open3d/_ml3d/torch/dataloaders/default_batcher.py:50: UserWarning: An output with one or more elements was resized since it had shape [8192], which does not match the required output shape [2, 256, 16]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:17.)\n",
      "  return torch.stack(batch, 0, out=out)\n",
      "/home/woutervdb/.local/lib/python3.10/site-packages/open3d/_ml3d/torch/dataloaders/default_batcher.py:50: UserWarning: An output with one or more elements was resized since it had shape [4096], which does not match the required output shape [2, 128, 16]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:17.)\n",
      "  return torch.stack(batch, 0, out=out)\n",
      "/home/woutervdb/.local/lib/python3.10/site-packages/open3d/_ml3d/torch/dataloaders/default_batcher.py:50: UserWarning: An output with one or more elements was resized since it had shape [131072], which does not match the required output shape [2, 65536, 1]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:17.)\n",
      "  return torch.stack(batch, 0, out=out)\n",
      "/home/woutervdb/.local/lib/python3.10/site-packages/open3d/_ml3d/torch/dataloaders/default_batcher.py:50: UserWarning: An output with one or more elements was resized since it had shape [32768], which does not match the required output shape [2, 16384, 1]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:17.)\n",
      "  return torch.stack(batch, 0, out=out)\n",
      "/home/woutervdb/.local/lib/python3.10/site-packages/open3d/_ml3d/torch/dataloaders/default_batcher.py:50: UserWarning: An output with one or more elements was resized since it had shape [8192], which does not match the required output shape [2, 4096, 1]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:17.)\n",
      "  return torch.stack(batch, 0, out=out)\n",
      "/home/woutervdb/.local/lib/python3.10/site-packages/open3d/_ml3d/torch/dataloaders/default_batcher.py:50: UserWarning: An output with one or more elements was resized since it had shape [2048], which does not match the required output shape [2, 1024, 1]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:17.)\n",
      "  return torch.stack(batch, 0, out=out)\n",
      "/home/woutervdb/.local/lib/python3.10/site-packages/open3d/_ml3d/torch/dataloaders/default_batcher.py:50: UserWarning: An output with one or more elements was resized since it had shape [512], which does not match the required output shape [2, 256, 1]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:17.)\n",
      "  return torch.stack(batch, 0, out=out)\n",
      "/home/woutervdb/.local/lib/python3.10/site-packages/open3d/_ml3d/torch/dataloaders/default_batcher.py:50: UserWarning: An output with one or more elements was resized since it had shape [131072], which does not match the required output shape [2, 65536]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:17.)\n",
      "  return torch.stack(batch, 0, out=out)\n",
      "training:   0%|          | 0/45 [00:14<?, ?it/s]\n"
     ]
    },
    {
     "ename": "OutOfMemoryError",
     "evalue": "CUDA out of memory. Tried to allocate 32.00 MiB (GPU 0; 4.00 GiB total capacity; 3.43 GiB already allocated; 0 bytes free; 3.46 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 11\u001b[0m\n\u001b[1;32m      8\u001b[0m pipeline \u001b[39m=\u001b[39m ml3d\u001b[39m.\u001b[39mpipelines\u001b[39m.\u001b[39mSemanticSegmentation(model, dataset, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mcfg\u001b[39m.\u001b[39mpipeline)\n\u001b[1;32m     10\u001b[0m \u001b[39m# Train model\u001b[39;00m\n\u001b[0;32m---> 11\u001b[0m pipeline\u001b[39m.\u001b[39;49mrun_train()\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/open3d/_ml3d/torch/pipelines/semantic_segmentation.py:409\u001b[0m, in \u001b[0;36mSemanticSegmentation.run_train\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    407\u001b[0m     inputs[\u001b[39m'\u001b[39m\u001b[39mdata\u001b[39m\u001b[39m'\u001b[39m]\u001b[39m.\u001b[39mto(device)\n\u001b[1;32m    408\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39moptimizer\u001b[39m.\u001b[39mzero_grad()\n\u001b[0;32m--> 409\u001b[0m results \u001b[39m=\u001b[39m model(inputs[\u001b[39m'\u001b[39;49m\u001b[39mdata\u001b[39;49m\u001b[39m'\u001b[39;49m])\n\u001b[1;32m    410\u001b[0m loss, gt_labels, predict_scores \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39mget_loss(\n\u001b[1;32m    411\u001b[0m     Loss, results, inputs, device)\n\u001b[1;32m    413\u001b[0m \u001b[39mif\u001b[39;00m predict_scores\u001b[39m.\u001b[39msize()[\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m] \u001b[39m==\u001b[39m \u001b[39m0\u001b[39m:\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1194\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1190\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1191\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1192\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1193\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1194\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1195\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1196\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/open3d/_ml3d/torch/models/randlanet.py:276\u001b[0m, in \u001b[0;36mRandLANet.forward\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m    274\u001b[0m encoder_feat_list \u001b[39m=\u001b[39m []\n\u001b[1;32m    275\u001b[0m \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(cfg\u001b[39m.\u001b[39mnum_layers):\n\u001b[0;32m--> 276\u001b[0m     feat_encoder_i \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mencoder[i](coords_list[i], feat,\n\u001b[1;32m    277\u001b[0m                                      neighbor_indices_list[i])\n\u001b[1;32m    278\u001b[0m     feat_sampled_i \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mrandom_sample(feat_encoder_i,\n\u001b[1;32m    279\u001b[0m                                         subsample_indices_list[i])\n\u001b[1;32m    280\u001b[0m     \u001b[39mif\u001b[39;00m i \u001b[39m==\u001b[39m \u001b[39m0\u001b[39m:\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1194\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1190\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1191\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1192\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1193\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1194\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1195\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1196\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/open3d/_ml3d/torch/models/randlanet.py:686\u001b[0m, in \u001b[0;36mLocalFeatureAggregation.forward\u001b[0;34m(self, coords, feat, neighbor_indices)\u001b[0m\n\u001b[1;32m    683\u001b[0m x, neighbor_features \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlse1(coords, x, neighbor_indices)\n\u001b[1;32m    684\u001b[0m x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpool1(x)\n\u001b[0;32m--> 686\u001b[0m x, _ \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mlse2(coords,\n\u001b[1;32m    687\u001b[0m                  x,\n\u001b[1;32m    688\u001b[0m                  neighbor_indices,\n\u001b[1;32m    689\u001b[0m                  relative_features\u001b[39m=\u001b[39;49mneighbor_features)\n\u001b[1;32m    690\u001b[0m x \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpool2(x)\n\u001b[1;32m    692\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlrelu(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmlp2(x) \u001b[39m+\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mshortcut(feat))\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1194\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1190\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1191\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1192\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1193\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1194\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1195\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1196\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/open3d/_ml3d/torch/models/randlanet.py:604\u001b[0m, in \u001b[0;36mLocalSpatialEncoding.forward\u001b[0;34m(self, coords, features, neighbor_indices, relative_features)\u001b[0m\n\u001b[1;32m    599\u001b[0m relative_features \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmlp(relative_features)\n\u001b[1;32m    601\u001b[0m neighbor_features \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mgather_neighbor(\n\u001b[1;32m    602\u001b[0m     features\u001b[39m.\u001b[39mtranspose(\u001b[39m1\u001b[39m, \u001b[39m2\u001b[39m)\u001b[39m.\u001b[39msqueeze(\u001b[39m3\u001b[39m), neighbor_indices)\n\u001b[0;32m--> 604\u001b[0m \u001b[39mreturn\u001b[39;00m torch\u001b[39m.\u001b[39;49mcat([neighbor_features, relative_features],\n\u001b[1;32m    605\u001b[0m                  dim\u001b[39m=\u001b[39;49m\u001b[39m1\u001b[39;49m), relative_features\n",
      "\u001b[0;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 32.00 MiB (GPU 0; 4.00 GiB total capacity; 3.43 GiB already allocated; 0 bytes free; 3.46 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF"
     ]
    }
   ],
   "source": [
    "# Load config file\n",
    "cfg_path = \"cfg/randlanet_leafwood.yml\"\n",
    "cfg = _ml3d.utils.Config.load_from_file(cfg_path)\n",
    "\n",
    "# Instantiate dataset, model and pipeline\n",
    "dataset = Custom3D(**cfg.dataset)\n",
    "model = ml3d.models.RandLANet(**cfg.model)\n",
    "pipeline = ml3d.pipelines.SemanticSegmentation(model, dataset, **cfg.pipeline)\n",
    "\n",
    "# Train model\n",
    "pipeline.run_train()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'normalize': 5, 'blabla': 7} 0\n"
     ]
    }
   ],
   "source": [
    "a = {\n",
    "    'recenter': 0,\n",
    "    'normalize': 5,\n",
    "    'blabla': 7,\n",
    "}\n",
    "b = a.pop('recenter')\n",
    "print(a, b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Jupyter environment detected. Enabling Open3D WebVisualizer.\n",
      "[Open3D INFO] WebRTC GUI backend enabled.\n",
      "[Open3D INFO] WebRTCWindowSystem: HTTP handshake server disabled.\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'dataset' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 29\u001b[0m\n\u001b[1;32m     26\u001b[0m ckpt_path \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39m../logs/CustomRandLANet_TropicalLeafWood_torch/checkpoint/ckpt_00040.pth\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m     27\u001b[0m pipeline\u001b[39m.\u001b[39mload_ckpt(ckpt_path)\n\u001b[0;32m---> 29\u001b[0m test_split \u001b[39m=\u001b[39m dataset\u001b[39m.\u001b[39mget_split(\u001b[39m\"\u001b[39m\u001b[39mtest\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m     30\u001b[0m idx\u001b[39m=\u001b[39m\u001b[39m5\u001b[39m\n\u001b[1;32m     31\u001b[0m data \u001b[39m=\u001b[39m test_split\u001b[39m.\u001b[39mget_data(idx)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'dataset' is not defined"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import open3d as o3d\n",
    "# import open3d.ml as _ml3d\n",
    "# import open3d.ml.torch as ml3d\n",
    "import ml3d as _ml3d\n",
    "import ml3d.torch as ml3d\n",
    "from open3d.ml.torch.datasets import Custom3D\n",
    "import numpy as np\n",
    "import glob\n",
    "import torch\n",
    "from pclbox.models import CustomRandLANet\n",
    "\n",
    "DATA_DIR = \"/mnt/c/Users/wavdnbro/OneDrive - UGent/Documents/spacetwin/datasets/leaf_wood/\"\n",
    "DATA_PATH = DATA_DIR + 'preprocessed/'\n",
    "\n",
    "# Load config file\n",
    "cfg_path = \"../cfg/randlanet_leafwood.yml\"\n",
    "cfg = _ml3d.utils.Config.load_from_file(cfg_path)\n",
    "\n",
    "dataset = Custom3D(**cfg.dataset)\n",
    "model = CustomRandLANet(**cfg.model)\n",
    "pipeline = ml3d.pipelines.SemanticSegmentation(model, dataset, **cfg.pipeline)\n",
    "\n",
    "ckpt_path = \"../logs/CustomRandLANet_TropicalLeafWood_torch/checkpoint/ckpt_00040.pth\"\n",
    "pipeline.load_ckpt(ckpt_path)\n",
    "\n",
    "test_split = dataset.get_split(\"test\")\n",
    "idx=5\n",
    "data = test_split.get_data(idx)\n",
    "data_prepr = model.preprocess(data, {'split': 'test'})\n",
    "\n",
    "print(data_prepr['search_tree'].data.shape)\n",
    "print(data['point'].shape)\n",
    "print(data_prepr['point'].shape)\n",
    "\n",
    "rng = np.random.default_rng(0)\n",
    "possibility = rng.random(data_prepr['point'].shape[0]) * 1e-3\n",
    "possibility.shape\n",
    "# result = pipeline.run_inference(data)\n",
    "# print(result)\n",
    "\n",
    "# prediction = np.hstack((data['point'], result['predict_labels'].reshape(-1, 1)))\n",
    "# np.savetxt('../tests/randlanet_prediction.txt', prediction, fmt='%.3f')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/278698 [00:31<?, ?it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(280382, 3)\n",
      "(278698, 3)\n",
      "dict_keys(['coords', 'neighbor_indices', 'sub_idx', 'interp_idx', 'features', 'point_inds', 'labels'])\n",
      "torch.Size([1, 65536, 3])\n",
      "torch.Size([1, 16384, 3])\n",
      "torch.Size([1, 4096, 3])\n",
      "torch.Size([1, 1024, 3])\n",
      "torch.Size([1, 256, 3])\n",
      "tensor([[202616, 111937,  75237,  ..., 181270,  96531, 209788]]) torch.Size([1, 65536])\n",
      "tensor([[[-2.0813,  2.1773],\n",
      "         [-2.4215,  2.3669],\n",
      "         [-7.0119,  6.7391],\n",
      "         ...,\n",
      "         [ 0.7758, -1.0117],\n",
      "         [-3.3071,  3.6906],\n",
      "         [ 0.5558, -0.8674]]], grad_fn=<TransposeBackward0>)\n",
      "[[1.3944362e-02 9.8605561e-01]\n",
      " [8.2565509e-03 9.9174339e-01]\n",
      " [1.0666932e-06 9.9999893e-01]\n",
      " ...\n",
      " [8.5662496e-01 1.4337496e-01]\n",
      " [9.1315957e-04 9.9908686e-01]\n",
      " [8.0584317e-01 1.9415690e-01]]\n",
      "[[[6.9721811e-04 4.9302783e-02]\n",
      "  [4.1282756e-04 4.9587172e-02]\n",
      "  [5.3334663e-08 4.9999949e-02]\n",
      "  ...\n",
      "  [4.2831250e-02 7.1687484e-03]\n",
      "  [4.5657980e-05 4.9954344e-02]\n",
      "  [4.0292159e-02 9.7078448e-03]]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.inference_begin(data)\n",
    "inputs = model.inference_preprocess()\n",
    "\n",
    "print(data['point'].shape)\n",
    "print(model.inference_data['point'].shape)\n",
    "print(inputs['data'].keys())\n",
    "print(inputs['data']['coords'][0].shape)\n",
    "print(inputs['data']['coords'][1].shape)\n",
    "print(inputs['data']['coords'][2].shape)\n",
    "print(inputs['data']['coords'][3].shape)\n",
    "print(inputs['data']['coords'][4].shape)\n",
    "inds = inputs['data']['point_inds']\n",
    "print(inds, inds.shape)\n",
    "\n",
    "model.device = 'cpu'\n",
    "results = model(inputs['data'])\n",
    "print(results)\n",
    "\n",
    "# print(model.test_probs)\n",
    "\n",
    "results = torch.reshape(results, (-1, 2))\n",
    "m_softmax = torch.nn.Softmax(dim=-1)\n",
    "results = m_softmax(results)\n",
    "results = results.cpu().data.numpy()\n",
    "probs = np.reshape(results, [-1, 2])\n",
    "\n",
    "print(probs)\n",
    "print(0.95 * model.test_probs[inds] + (1 - 0.95) * probs)\n",
    "\n",
    "model.possibility[model.possibility > 0.5].shape[0]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "# --- Check inference dataloading and result\n",
    "\n",
    "from ml3d.datasets import InferenceDummySplit\n",
    "from ml3d.torch.dataloaders import TorchDataloader, get_sampler\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# Code copied from pipeline.run_inference()\n",
    "model.device = 'cpu'\n",
    "batcher = pipeline.get_batcher(model.device)\n",
    "infer_dataset = InferenceDummySplit(data)\n",
    "pipeline.dataset_split = infer_dataset\n",
    "infer_sampler = infer_dataset.sampler\n",
    "infer_split = TorchDataloader(dataset=infer_dataset,\n",
    "                                preprocess=model.preprocess,\n",
    "                                transform=model.transform,\n",
    "                                sampler=infer_sampler,\n",
    "                                use_cache=False)\n",
    "infer_loader = DataLoader(infer_split,\n",
    "                            batch_size=1,\n",
    "                            sampler=get_sampler(infer_sampler),\n",
    "                            collate_fn=batcher.collate_fn)\n",
    "\n",
    "model.trans_point_sampler = infer_sampler.get_point_sampler()\n",
    "\n",
    "c = 0\n",
    "for _, inputs in enumerate(infer_loader):\n",
    "    print(infer_sampler.cloud_id)\n",
    "    # make prediction\n",
    "    results = model(inputs['data'])\n",
    "\n",
    "    # Softmax to get probabilities\n",
    "    results = torch.reshape(results, (-1, 2))\n",
    "    m_softmax = torch.nn.Softmax(dim=-1)\n",
    "    results = m_softmax(results)\n",
    "    results = results.cpu().data.numpy()\n",
    "\n",
    "    # Argmax to get labels\n",
    "    probs = np.reshape(results, [-1, 2])\n",
    "    labels = np.argmax(probs, 1).reshape(-1, 1)\n",
    "\n",
    "    # Save sub point clouds with predicted label\n",
    "    prediction = np.hstack((inputs['data']['coords'][0][0].numpy(), labels))\n",
    "    np.savetxt('../tests/randlanet_prediction' + str(c) + '.txt', prediction, fmt='%.3f')\n",
    "    c += 1\n",
    "\n",
    "\n",
    "\n",
    "# results are quite ok so will have to find where it goes wrong with saving for entire cloud"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1. 1. 1. ... 1. 1. 1.]\n",
      "[ 693.5 2048. ]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([3.4161, 1.3382])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "print(np.sum(result['predict_scores'], axis=1))\n",
    "print(np.sum(result['predict_scores'], axis=0))\n",
    "\n",
    "def get_class_weights(num_per_class):\n",
    "        num_per_class = np.array(num_per_class, dtype=np.float32)\n",
    "        weight = num_per_class / float(sum(num_per_class))\n",
    "        ce_label_weight = 1 / (weight + 0.02)\n",
    "        return torch.tensor(ce_label_weight)\n",
    "\n",
    "get_class_weights([3000, 8000])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAlUAAAGwCAYAAACAZ5AeAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjcuMSwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/bCgiHAAAACXBIWXMAAA9hAAAPYQGoP6dpAAA+Y0lEQVR4nO3deXhN997//1cSMiEDmqlNUXMO5ZAihpbKaZS6Tedb89AGHZIWqTq0ilbbKKUo5eggepUazinHjYY0hCNSQ4hZaGlpSaghISqJZP3+6C/rtitaiZVh83xc174ue33ea633+tCzX2fttdZ2MAzDEAAAAO6IY1k3AAAAcDcgVAEAAFiAUAUAAGABQhUAAIAFCFUAAAAWIFQBAABYgFAFAABggQpl3cC9JD8/X6dPn1aVKlXk4OBQ1u0AAIDbYBiGLl++rICAADk63vp8FKGqFJ0+fVqBgYFl3QYAACiGU6dO6YEHHrjlOKGqFFWpUkXSb38pHh4eZdwNAAC4HZmZmQoMDDQ/x2+FUFWKCr7y8/DwIFQBAGBn/uzSHS5UBwAAsAChCgAAwAKEKgAAAAsQqgAAACxAqAIAALAAoQoAAMAChCoAAAALEKoAAAAsQKgCAACwAKEKAADAAoQqAAAACxCqAAAALECoAgAAsAChCgAAwAKEKgAAAAtUKOsGAAAATJuii79uh3HW9VEMnKkCAACwAGeq7hIfxB29o/VH/a2eRZ0AAHBv4kwVAACABQhVAAAAFiBUAQAAWIBQBQAAYAFCFQAAgAUIVQAAABYgVAEAAFiAUAUAAGABQhUAAIAFCFUAAAAWIFQBAABYgFAFAABgAUIVAACABQhVAAAAFiBUAQAAWIBQBQAAYAFCFQAAgAUIVQAAABYgVAEAAFiAUAUAAGABQhUAAIAFyjRUbdmyRV27dlVAQIAcHBy0atUqm3HDMDRhwgT5+/vLzc1NoaGhOnbsmE3NhQsX1L9/f3l4eMjLy0vh4eG6cuWKTc2+ffvUrl07ubq6KjAwUFOnTr2plxUrVqhBgwZydXVV48aNtW7duiL3AgAA7l1lGqqysrLUpEkTzZ07t9DxqVOnavbs2Zo/f762b9+uSpUqKSwsTNeuXTNr+vfvr4MHDyouLk5r1qzRli1bNHz4cHM8MzNTTzzxhGrUqKHk5GRNmzZNkyZN0oIFC8yabdu2qW/fvgoPD9eePXvUvXt3de/eXQcOHChSLwAA4N7lYBiGUdZNSJKDg4NWrlyp7t27S/rtzFBAQIBeeeUVjR49WpKUkZEhX19fxcTEqE+fPjp8+LCCgoK0c+dOBQcHS5JiY2PVuXNn/fTTTwoICNC8efP0+uuvKy0tTc7OzpKksWPHatWqVTpy5IgkqXfv3srKytKaNWvMflq1aqWmTZtq/vz5t9XL7cjMzJSnp6cyMjLk4eFhybwV+CDu6B2tP+pv9SzqBACAO7ApuvjrdhhnXR83uN3P73J7TdWJEyeUlpam0NBQc5mnp6datmyppKQkSVJSUpK8vLzMQCVJoaGhcnR01Pbt282aRx991AxUkhQWFqbU1FRdvHjRrLlxPwU1Bfu5nV4Kk52drczMTJsXAAC4O5XbUJWWliZJ8vX1tVnu6+trjqWlpcnHx8dmvEKFCqpatapNTWHbuHEft6q5cfzPeilMdHS0PD09zVdgYOCfHDUAALBX5TZU3Q3GjRunjIwM83Xq1KmybgkAAJSQchuq/Pz8JEnp6ek2y9PT080xPz8/nT171mb8+vXrunDhgk1NYdu4cR+3qrlx/M96KYyLi4s8PDxsXgAA4O5UbkNVrVq15Ofnp/j4eHNZZmamtm/frpCQEElSSEiILl26pOTkZLNm48aNys/PV8uWLc2aLVu2KDc316yJi4tT/fr15e3tbdbcuJ+CmoL93E4vAADg3lamoerKlStKSUlRSkqKpN8uCE9JSdHJkyfl4OCgkSNH6u2339bq1au1f/9+DRo0SAEBAeYdgg0bNlSnTp00bNgw7dixQ4mJiYqMjFSfPn0UEBAgSerXr5+cnZ0VHh6ugwcPatmyZZo1a5aioqLMPkaMGKHY2FhNnz5dR44c0aRJk7Rr1y5FRkZK0m31AgAA7m0VynLnu3btUocOHcz3BUFn8ODBiomJ0ZgxY5SVlaXhw4fr0qVLatu2rWJjY+Xq6mqus3jxYkVGRqpjx45ydHRUr169NHv2bHPc09NTGzZsUEREhJo3b67q1atrwoQJNs+yat26tZYsWaLx48frtddeU926dbVq1So1atTIrLmdXgAAwL2r3Dyn6l7Ac6oAAPgTPKcKAADg3kaoAgAAsAChCgAAwAKEKgAAAAsQqgAAACxAqAIAALAAoQoAAMAChCoAAAALEKoAAAAsQKgCAACwAKEKAADAAoQqAAAACxCqAAAALECoAgAAsAChCgAAwAKEKgAAAAsQqgAAACxAqAIAALAAoQoAAMAChCoAAAALEKoAAAAsQKgCAACwAKEKAADAAoQqAAAACxCqAAAALECoAgAAsAChCgAAwAKEKgAAAAsQqgAAACxAqAIAALAAoQoAAMAChCoAAAALEKoAAAAsQKgCAACwAKEKAADAAoQqAAAACxCqAAAALECoAgAAsAChCgAAwAKEKgAAAAsQqgAAACxAqAIAALAAoQoAAMAChCoAAAALEKoAAAAsQKgCAACwAKEKAADAAoQqAAAACxCqAAAALECoAgAAsAChCgAAwAKEKgAAAAsQqgAAACxAqAIAALBAuQ5VeXl5euONN1SrVi25ubmpdu3amjx5sgzDMGsMw9CECRPk7+8vNzc3hYaG6tixYzbbuXDhgvr37y8PDw95eXkpPDxcV65csanZt2+f2rVrJ1dXVwUGBmrq1Kk39bNixQo1aNBArq6uaty4sdatW1cyBw4AAOxOuQ5V7733nubNm6c5c+bo8OHDeu+99zR16lR9+OGHZs3UqVM1e/ZszZ8/X9u3b1elSpUUFhama9eumTX9+/fXwYMHFRcXpzVr1mjLli0aPny4OZ6ZmaknnnhCNWrUUHJysqZNm6ZJkyZpwYIFZs22bdvUt29fhYeHa8+ePerevbu6d++uAwcOlM5kAACAcs3BuPG0Tznz1FNPydfXV59++qm5rFevXnJzc9MXX3whwzAUEBCgV155RaNHj5YkZWRkyNfXVzExMerTp48OHz6soKAg7dy5U8HBwZKk2NhYde7cWT/99JMCAgI0b948vf7660pLS5Ozs7MkaezYsVq1apWOHDkiSerdu7eysrK0Zs0as5dWrVqpadOmmj9//m0dT2Zmpjw9PZWRkSEPDw9L5qjAB3FH72j9UX+rZ1EnAADcgU3RxV+3wzjr+rjB7X5+l+szVa1bt1Z8fLyOHv0tMOzdu1dbt27Vk08+KUk6ceKE0tLSFBoaaq7j6empli1bKikpSZKUlJQkLy8vM1BJUmhoqBwdHbV9+3az5tFHHzUDlSSFhYUpNTVVFy9eNGtu3E9BTcF+CpOdna3MzEybFwAAuDtVKOsG/sjYsWOVmZmpBg0ayMnJSXl5eXrnnXfUv39/SVJaWpokydfX12Y9X19fcywtLU0+Pj424xUqVFDVqlVtamrVqnXTNgrGvL29lZaW9of7KUx0dLTefPPNoh42AACwQ+X6TNXy5cu1ePFiLVmyRLt379aiRYv0/vvva9GiRWXd2m0ZN26cMjIyzNepU6fKuiUAAFBCyvWZqldffVVjx45Vnz59JEmNGzfWjz/+qOjoaA0ePFh+fn6SpPT0dPn7+5vrpaenq2nTppIkPz8/nT171ma7169f14ULF8z1/fz8lJ6eblNT8P7PagrGC+Pi4iIXF5eiHjYAALBD5fpM1dWrV+XoaNuik5OT8vPzJUm1atWSn5+f4uPjzfHMzExt375dISEhkqSQkBBdunRJycnJZs3GjRuVn5+vli1bmjVbtmxRbm6uWRMXF6f69evL29vbrLlxPwU1BfsBAAD3tnIdqrp27ap33nlHa9eu1Q8//KCVK1dqxowZ6tGjhyTJwcFBI0eO1Ntvv63Vq1dr//79GjRokAICAtS9e3dJUsOGDdWpUycNGzZMO3bsUGJioiIjI9WnTx8FBARIkvr16ydnZ2eFh4fr4MGDWrZsmWbNmqWoqCizlxEjRig2NlbTp0/XkSNHNGnSJO3atUuRkZGlPi8AAKD8Kddf/3344Yd644039OKLL+rs2bMKCAjQc889pwkTJpg1Y8aMUVZWloYPH65Lly6pbdu2io2Nlaurq1mzePFiRUZGqmPHjnJ0dFSvXr00e/Zsc9zT01MbNmxQRESEmjdvrurVq2vChAk2z7Jq3bq1lixZovHjx+u1115T3bp1tWrVKjVq1Kh0JgMAAJRr5fo5VXcbnlMFAMCf4DlVAAAA9zZCFQAAgAUIVQAAABYgVAEAAFiAUAUAAGABQhUAAIAFCFUAAAAWIFQBAABYgFAFAABgAUIVAACABQhVAAAAFiBUAQAAWIBQBQAAYAFCFQAAgAUIVQAAABYgVAEAAFiAUAUAAGABQhUAAIAFCFUAAAAWIFQBAABYgFAFAABgAUIVAACABQhVAAAAFiBUAQAAWIBQBQAAYAFCFQAAgAUIVQAAABYgVAEAAFiAUAUAAGABQhUAAIAFCFUAAAAWIFQBAABYgFAFAABgAUIVAACABQhVAAAAFihWqDp+/LjVfQAAANi1YoWqOnXqqEOHDvriiy907do1q3sCAACwO8UKVbt379bDDz+sqKgo+fn56bnnntOOHTus7g0AAMBuFCtUNW3aVLNmzdLp06f12Wef6cyZM2rbtq0aNWqkGTNm6Ny5c1b3CQAAUK7d0YXqFSpUUM+ePbVixQq99957+u677zR69GgFBgZq0KBBOnPmjFV9AgAAlGt3FKp27dqlF198Uf7+/poxY4ZGjx6t77//XnFxcTp9+rS6detmVZ8AAADlWoXirDRjxgwtXLhQqamp6ty5sz7//HN17txZjo6/ZbRatWopJiZGNWvWtLJXAACAcqtYoWrevHl69tlnNWTIEPn7+xda4+Pjo08//fSOmgMAALAXxQpVx44d+9MaZ2dnDR48uDibBwAAsDvFuqZq4cKFWrFixU3LV6xYoUWLFt1xUwAAAPamWKEqOjpa1atXv2m5j4+P3n333TtuCgAAwN4UK1SdPHlStWrVuml5jRo1dPLkyTtuCgAAwN4UK1T5+Pho3759Ny3fu3evqlWrdsdNAQAA2Jtihaq+ffvq5Zdf1qZNm5SXl6e8vDxt3LhRI0aMUJ8+fazuEQAAoNwr1t1/kydP1g8//KCOHTuqQoXfNpGfn69BgwZxTRUAALgnFStUOTs7a9myZZo8ebL27t0rNzc3NW7cWDVq1LC6PwAAALtQrFBVoF69eqpXr55VvQAAANitYoWqvLw8xcTEKD4+XmfPnlV+fr7N+MaNGy1pDgAAwF4UK1SNGDFCMTEx6tKlixo1aiQHBwer+wIAALArxbr7b+nSpVq+fLmWLVummTNn6oMPPrB5Wennn3/WgAEDVK1aNfParV27dpnjhmFowoQJ8vf3l5ubm0JDQ2/6GZ0LFy6of//+8vDwkJeXl8LDw3XlyhWbmn379qldu3ZydXVVYGCgpk6delMvK1asUIMGDeTq6qrGjRtr3bp1lh4rAACwX8UKVc7OzqpTp47Vvdzk4sWLatOmjSpWrKivv/5ahw4d0vTp0+Xt7W3WTJ06VbNnz9b8+fO1fft2VapUSWFhYbp27ZpZ079/fx08eFBxcXFas2aNtmzZouHDh5vjmZmZeuKJJ1SjRg0lJydr2rRpmjRpkhYsWGDWbNu2TX379lV4eLj27Nmj7t27q3v37jpw4ECJzwMAACj/HAzDMIq60vTp03X8+HHNmTOnRL/6Gzt2rBITE/Xf//630HHDMBQQEKBXXnlFo0ePliRlZGTI19dXMTEx6tOnjw4fPqygoCDt3LlTwcHBkqTY2Fh17txZP/30kwICAjRv3jy9/vrrSktLk7Ozs7nvVatW6ciRI5Kk3r17KysrS2vWrDH336pVKzVt2lTz588vtL/s7GxlZ2eb7zMzMxUYGKiMjAx5eHjc+QTd4IO4o3e0/qi/ccMBAKAc2BRd/HU7jLOujxtkZmbK09PzTz+/i3WmauvWrVq8eLFq166trl27qmfPnjYvq6xevVrBwcH6f//v/8nHx0d//etf9fHHH5vjJ06cUFpamkJDQ81lnp6eatmypZKSkiRJSUlJ8vLyMgOVJIWGhsrR0VHbt283ax599FEzUElSWFiYUlNTdfHiRbPmxv0U1BTspzDR0dHy9PQ0X4GBgXcwGwAAoDwrVqjy8vJSjx499Nhjj6l69eo2wcHT09Oy5o4fP6558+apbt26Wr9+vV544QW9/PLLWrRokSQpLS1NkuTr62uznq+vrzmWlpYmHx8fm/EKFSqoatWqNjWFbePGfdyqpmC8MOPGjVNGRob5OnXqVJGOHwAA2I9i3f23cOFCq/soVH5+voKDg82ntP/1r3/VgQMHNH/+fA0ePLhUergTLi4ucnFxKes2AABAKSjWmSpJun79ur755hv985//1OXLlyVJp0+fvumuujvh7++voKAgm2UNGzbUyZMnJUl+fn6SpPT0dJua9PR0c8zPz09nz569qfcLFy7Y1BS2jRv3cauagnEAAHBvK1ao+vHHH9W4cWN169ZNEREROnfunCTpvffeMy8Yt0KbNm2Umppqs+zo0aPmz+HUqlVLfn5+io+PN8czMzO1fft2hYSESJJCQkJ06dIlJScnmzUbN25Ufn6+WrZsadZs2bJFubm5Zk1cXJzq169v3mkYEhJis5+CmoL9AACAe1uxQtWIESMUHBysixcvys3NzVzeo0ePm4LHnRg1apS+/fZbvfvuu/ruu++0ZMkSLViwQBEREZIkBwcHjRw5Um+//bZWr16t/fv3a9CgQQoICFD37t0l/XZmq1OnTho2bJh27NihxMRERUZGqk+fPgoICJAk9evXT87OzgoPD9fBgwe1bNkyzZo1S1FRUTbHHBsbq+nTp+vIkSOaNGmSdu3apcjISMuOFwAA2K9iXVP13//+V9u2bbO5W06SatasqZ9//tmSxiTpkUce0cqVKzVu3Di99dZbqlWrlmbOnKn+/fubNWPGjFFWVpaGDx+uS5cuqW3btoqNjZWrq6tZs3jxYkVGRqpjx45ydHRUr169NHv2bHPc09NTGzZsUEREhJo3b67q1atrwoQJNs+yat26tZYsWaLx48frtddeU926dbVq1So1atTIsuMFAAD2q1jPqfL29lZiYqKCgoJUpUoV7d27Vw899JC2bt2qXr163XTtEX5zu8+5KA6eUwUAuCvca8+peuKJJzRz5kzzvYODg65cuaKJEyeqc+fOxdkkAACAXSvW13/Tp09XWFiYgoKCdO3aNfXr10/Hjh1T9erV9eWXX1rdIwAAQLlXrFD1wAMPaO/evVq6dKn27dunK1euKDw8XP3797e5cB0AAOBeUaxQJf32VPIBAwZY2QsAAIDdKlao+vzzz/9wfNCgQcVqBgAAwF4VK1SNGDHC5n1ubq6uXr0qZ2dnubu7E6oAAMA9p1h3/128eNHmdeXKFaWmpqpt27ZcqA4AAO5Jxf7tv9+rW7eupkyZctNZLAAAgHuBZaFK+u3i9dOnT1u5SQAAALtQrGuqVq9ebfPeMAydOXNGc+bMUZs2bSxpDAAAwJ4UK1QV/FhxAQcHB9133316/PHHNX36dCv6AgAAsCvFClX5+flW9wEAAGDXLL2mCgAA4F5VrDNVUVFRt107Y8aM4uwCAADArhQrVO3Zs0d79uxRbm6u6tevL0k6evSonJyc1KxZM7POwcHBmi4BAADKuWKFqq5du6pKlSpatGiRvL29Jf32QNBnnnlG7dq10yuvvGJpkwAAAOVdsa6pmj59uqKjo81AJUne3t56++23ufsPAADck4oVqjIzM3Xu3Lmblp87d06XL1++46YAAADsTbFCVY8ePfTMM8/oq6++0k8//aSffvpJ//73vxUeHq6ePXta3SMAAEC5V6xrqubPn6/Ro0erX79+ys3N/W1DFSooPDxc06ZNs7RBAAAAe1CsUOXu7q6PPvpI06ZN0/fffy9Jql27tipVqmRpcwAAAPbijh7+eebMGZ05c0Z169ZVpUqVZBiGVX0BAADYlWKFqvPnz6tjx46qV6+eOnfurDNnzkiSwsPDeZwCAAC4JxUrVI0aNUoVK1bUyZMn5e7ubi7v3bu3YmNjLWsOAADAXhTrmqoNGzZo/fr1euCBB2yW161bVz/++KMljQEAANiTYp2pysrKsjlDVeDChQtycXG546YAAADsTbFCVbt27fT555+b7x0cHJSfn6+pU6eqQ4cOljUHAABgL4r19d/UqVPVsWNH7dq1Szk5ORozZowOHjyoCxcuKDEx0eoeAQAAyr1inalq1KiRjh49qrZt26pbt27KyspSz549tWfPHtWuXdvqHgEAAMq9Ip+pys3NVadOnTR//ny9/vrrJdETAACA3SnymaqKFStq3759JdELAACA3SrW138DBgzQp59+anUvAAAAdqtYF6pfv35dn332mb755hs1b978pt/8mzFjhiXNAQAA2Isiharjx4+rZs2aOnDggJo1ayZJOnr0qE2Ng4ODdd0BAADYiSKFqrp16+rMmTPatGmTpN9+lmb27Nny9fUtkeYAAADsRZGuqTIMw+b9119/raysLEsbAgAAsEfFulC9wO9DFgAAwL2qSKHKwcHhpmumuIYKAACgiNdUGYahIUOGmD+afO3aNT3//PM33f331VdfWdchAACAHShSqBo8eLDN+wEDBljaDAAAgL0qUqhauHBhSfUBAABg1+7oQnUAAAD8hlAFAABgAUIVAACABQhVAAAAFiBUAQAAWIBQBQAAYAFCFQAAgAUIVQAAABYgVAEAAFiAUAUAAGABQhUAAIAFCFUAAAAWIFQBAABYwK5C1ZQpU+Tg4KCRI0eay65du6aIiAhVq1ZNlStXVq9evZSenm6z3smTJ9WlSxe5u7vLx8dHr776qq5fv25Tk5CQoGbNmsnFxUV16tRRTEzMTfufO3euatasKVdXV7Vs2VI7duwoicMEAAB2yG5C1c6dO/XPf/5TDz/8sM3yUaNG6X//93+1YsUKbd68WadPn1bPnj3N8by8PHXp0kU5OTnatm2bFi1apJiYGE2YMMGsOXHihLp06aIOHTooJSVFI0eO1NChQ7V+/XqzZtmyZYqKitLEiRO1e/duNWnSRGFhYTp79mzJHzwAACj37CJUXblyRf3799fHH38sb29vc3lGRoY+/fRTzZgxQ48//riaN2+uhQsXatu2bfr2228lSRs2bNChQ4f0xRdfqGnTpnryySc1efJkzZ07Vzk5OZKk+fPnq1atWpo+fboaNmyoyMhI/f3vf9cHH3xg7mvGjBkaNmyYnnnmGQUFBWn+/Plyd3fXZ599dsu+s7OzlZmZafMCAAB3J7sIVREREerSpYtCQ0NtlicnJys3N9dmeYMGDfTggw8qKSlJkpSUlKTGjRvL19fXrAkLC1NmZqYOHjxo1vx+22FhYeY2cnJylJycbFPj6Oio0NBQs6Yw0dHR8vT0NF+BgYHFnAEAAFDelftQtXTpUu3evVvR0dE3jaWlpcnZ2VleXl42y319fZWWlmbW3BioCsYLxv6oJjMzU7/++qt++eUX5eXlFVpTsI3CjBs3ThkZGebr1KlTt3fQAADA7lQo6wb+yKlTpzRixAjFxcXJ1dW1rNspMhcXF7m4uJR1GwAAoBSU6zNVycnJOnv2rJo1a6YKFSqoQoUK2rx5s2bPnq0KFSrI19dXOTk5unTpks166enp8vPzkyT5+fnddDdgwfs/q/Hw8JCbm5uqV68uJyenQmsKtgEAAO5t5TpUdezYUfv371dKSor5Cg4OVv/+/c0/V6xYUfHx8eY6qampOnnypEJCQiRJISEh2r9/v81denFxcfLw8FBQUJBZc+M2CmoKtuHs7KzmzZvb1OTn5ys+Pt6sAQAA97Zy/fVflSpV1KhRI5tllSpVUrVq1czl4eHhioqKUtWqVeXh4aGXXnpJISEhatWqlSTpiSeeUFBQkAYOHKipU6cqLS1N48ePV0REhPnV3PPPP685c+ZozJgxevbZZ7Vx40YtX75ca9euNfcbFRWlwYMHKzg4WC1atNDMmTOVlZWlZ555ppRmAwAAlGflOlTdjg8++ECOjo7q1auXsrOzFRYWpo8++sgcd3Jy0po1a/TCCy8oJCRElSpV0uDBg/XWW2+ZNbVq1dLatWs1atQozZo1Sw888IA++eQThYWFmTW9e/fWuXPnNGHCBKWlpalp06aKjY296eJ1AABwb3IwDMMo6ybuFZmZmfL09FRGRoY8PDws3fYHcUfvaP1Rf6tnUScAANyBTTff7X/bOoyzro8b3O7nd7m+pgoAAMBeEKoAAAAsQKgCAACwAKEKAADAAoQqAAAACxCqAAAALECoAgAAsAChCgAAwAKEKgAAAAsQqgAAACxAqAIAALAAoQoAAMAChCoAAAALEKoAAAAsQKgCAACwAKEKAADAAoQqAAAACxCqAAAALECoAgAAsAChCgAAwAKEKgAAAAsQqgAAACxAqAIAALAAoQoAAMAChCoAAAALEKoAAAAsQKgCAACwAKEKAADAAoQqAAAACxCqAAAALECoAgAAsAChCgAAwAKEKgAAAAsQqgAAACxAqAIAALAAoQoAAMAChCoAAAALEKoAAAAsQKgCAACwAKEKAADAAoQqAAAACxCqAAAALECoAgAAsAChCgAAwAKEKgAAAAsQqgAAACxAqAIAALAAoQoAAMAChCoAAAALEKoAAAAsQKgCAACwAKEKAADAAoQqAAAAC5TrUBUdHa1HHnlEVapUkY+Pj7p3767U1FSbmmvXrikiIkLVqlVT5cqV1atXL6Wnp9vUnDx5Ul26dJG7u7t8fHz06quv6vr16zY1CQkJatasmVxcXFSnTh3FxMTc1M/cuXNVs2ZNubq6qmXLltqxY4flxwwAAOxTuQ5VmzdvVkREhL799lvFxcUpNzdXTzzxhLKyssyaUaNG6X//93+1YsUKbd68WadPn1bPnj3N8by8PHXp0kU5OTnatm2bFi1apJiYGE2YMMGsOXHihLp06aIOHTooJSVFI0eO1NChQ7V+/XqzZtmyZYqKitLEiRO1e/duNWnSRGFhYTp79mzpTAYAACjXHAzDMMq6idt17tw5+fj4aPPmzXr00UeVkZGh++67T0uWLNHf//53SdKRI0fUsGFDJSUlqVWrVvr666/11FNP6fTp0/L19ZUkzZ8/X//4xz907tw5OTs76x//+IfWrl2rAwcOmPvq06ePLl26pNjYWElSy5Yt9cgjj2jOnDmSpPz8fAUGBuqll17S2LFjb6v/zMxMeXp6KiMjQx4eHlZOjT6IO3pH64/6Wz2LOgEA4A5sii7+uh3GWdfHDW7387tcn6n6vYyMDElS1apVJUnJycnKzc1VaGioWdOgQQM9+OCDSkpKkiQlJSWpcePGZqCSpLCwMGVmZurgwYNmzY3bKKgp2EZOTo6Sk5NtahwdHRUaGmrWFCY7O1uZmZk2LwAAcHeym1CVn5+vkSNHqk2bNmrUqJEkKS0tTc7OzvLy8rKp9fX1VVpamllzY6AqGC8Y+6OazMxM/frrr/rll1+Ul5dXaE3BNgoTHR0tT09P8xUYGFj0AwcAAHbBbkJVRESEDhw4oKVLl5Z1K7dt3LhxysjIMF+nTp0q65YAAEAJqVDWDdyOyMhIrVmzRlu2bNEDDzxgLvfz81NOTo4uXbpkc7YqPT1dfn5+Zs3v79IruDvwxprf3zGYnp4uDw8Pubm5ycnJSU5OToXWFGyjMC4uLnJxcSn6AZeBO7kmi+uxAAAo52eqDMNQZGSkVq5cqY0bN6pWrVo2482bN1fFihUVHx9vLktNTdXJkycVEhIiSQoJCdH+/ftt7tKLi4uTh4eHgoKCzJobt1FQU7ANZ2dnNW/e3KYmPz9f8fHxZg0AALi3leszVREREVqyZIn+85//qEqVKub1S56ennJzc5Onp6fCw8MVFRWlqlWrysPDQy+99JJCQkLUqlUrSdITTzyhoKAgDRw4UFOnTlVaWprGjx+viIgI8yzS888/rzlz5mjMmDF69tlntXHjRi1fvlxr1641e4mKitLgwYMVHBysFi1aaObMmcrKytIzzzxT+hMDAEB5did38Nmxch2q5s2bJ0lq3769zfKFCxdqyJAhkqQPPvhAjo6O6tWrl7KzsxUWFqaPPvrIrHVyctKaNWv0wgsvKCQkRJUqVdLgwYP11ltvmTW1atXS2rVrNWrUKM2aNUsPPPCAPvnkE4WFhZk1vXv31rlz5zRhwgSlpaWpadOmio2NvenidQAAcG+yq+dU2bvy/JyqO8E1VQAAG2V1pornVAEAANg/QhUAAIAFCFUAAAAWIFQBAABYoFzf/QcAkpSXl6fc3NyybuOuUbFiRTk5OZV1G8Bdh1AFoNwyDENpaWm6dOlSWbdy1/Hy8pKfn58cHBzKuhXgrkGoAlBuFQQqHx8fubu7EwAsYBiGrl69av7KhL+/fxl3BNw9CFUAyqW8vDwzUFWrVq2s27mruLm5SZLOnj0rHx8fvgoELMKF6gDKpYJrqNzd3cu4k7tTwbxyrRpgHUIVgHKNr/xKBvMKWI9QBQAAYAFCFQBYrH379ho5cqRl2zMMQ8OHD1fVqlXl4OCglJQUy7YNwDpcqA7A7pT2D4iX9Y+Gx8bGKiYmRgkJCXrooYdUvXr1Mu0HQOEIVQBQzn3//ffy9/dX69aty7oVAH+Ar/8AoARlZ2dr9OjRuv/++1WpUiW1bNlSCQkJ5vj58+fVt29f3X///XJ3d1fjxo315ZdfmuNDhgzRSy+9pJMnT8rBwUE1a9Ys/YMAcFs4UwUAJSgyMlKHDh3S0qVLFRAQoJUrV6pTp07av3+/6tatq2vXrql58+b6xz/+IQ8PD61du1YDBw5U7dq11aJFC82aNUu1a9fWggULtHPnTp4pBZRjhCoAKCEnT57UwoULdfLkSQUEBEiSRo8erdjYWC1cuFDvvvuu7r//fo0ePdpc56WXXtL69eu1fPlytWjRQp6enqpSpYqcnJzk5+dXVocC4DYQqgCghOzfv195eXmqV8/2Qvfs7GzzKfF5eXl69913tXz5cv3888/KyclRdnY2Dz0F7BChCgBKyJUrV+Tk5KTk5OSbvrarXLmyJGnatGmaNWuWZs6cqcaNG6tSpUoaOXKkcnJyyqJlAHeAUAUAJeSvf/2r8vLydPbsWbVr167QmsTERHXr1k0DBgyQJOXn5+vo0aMKCgoqzVYBWIC7/wCghNSrV0/9+/fXoEGD9NVXX+nEiRPasWOHoqOjtXbtWklS3bp1FRcXp23btunw4cN67rnnlJ6eXsadAygOzlQBsDtl/TDOoli4cKHefvttvfLKK/r5559VvXp1tWrVSk899ZQkafz48Tp+/LjCwsLk7u6u4cOHq3v37srIyCjjzgEUlYNhGEZZN3GvyMzMlKenpzIyMuTh4WHptkv7CdM3sqcPONiPa9eu6cSJE6pVq5ZcXV3Lup27DvOLErUpumz222FciWz2dj+/+foPAADAAoQqAAAACxCqAAAALECoAgAAsAChCgAAwAKEKgAAAAsQqgAAACxAqAIAALAAoQoAAMAChCoAuMvExMTIy8urrNsA7jn89h8A+1PaP4FRQj99AeDuwpkqAAAACxCqAMBCa9askZeXl/Ly8iRJKSkpcnBw0NixY82aoUOHasCAAZKkf//73/rLX/4iFxcX1axZU9OnT7fZ3sWLFzVo0CB5e3vL3d1dTz75pI4dO2ZTExMTowcffFDu7u7q0aOHzp8/X8JHCaAwhCoAsFC7du10+fJl7dmzR5K0efNmVa9eXQkJCWbN5s2b1b59eyUnJ+vpp59Wnz59tH//fk2aNElvvPGGYmJizNohQ4Zo165dWr16tZKSkmQYhjp37qzc3FxJ0vbt2xUeHq7IyEilpKSoQ4cOevvtt0vzkAH8/whVAGAhT09PNW3a1AxRCQkJGjVqlPbs2aMrV67o559/1nfffafHHntMM2bMUMeOHfXGG2+oXr16GjJkiCIjIzVt2jRJ0rFjx7R69Wp98sknateunZo0aaLFixfr559/1qpVqyRJs2bNUqdOnTRmzBjVq1dPL7/8ssLCwsro6IF7G6EKACz22GOPKSEhQYZh6L///a969uyphg0bauvWrdq8ebMCAgJUt25dHT58WG3atLFZt02bNjp27Jjy8vJ0+PBhVahQQS1btjTHq1Wrpvr16+vw4cOSpMOHD9uMS1JISEjJHySAm3D3HwBYrH379vrss8+0d+9eVaxYUQ0aNFD79u2VkJCgixcv6rHHHivrFgGUAM5UAYDFCq6r+uCDD8wAVRCqEhIS1L59e0lSw4YNlZiYaLNuYmKi6tWrJycnJzVs2FDXr1/X9u3bzfHz588rNTVVQUFB5jZuHJekb7/9tgSPDsCtEKoAwGLe3t56+OGHtXjxYjNAPfroo9q9e7eOHj1qBq1XXnlF8fHxmjx5so4ePapFixZpzpw5Gj16tCSpbt266tatm4YNG6atW7dq7969GjBggO6//35169ZNkvTyyy8rNjZW77//vo4dO6Y5c+YoNja2TI4buNfx9R8A+2MHD+N87LHHlJKSYoaqqlWrKigoSOnp6apfv74kqVmzZlq+fLkmTJigyZMny9/fX2+99ZaGDBlibmfhwoUaMWKEnnrqKeXk5OjRRx/VunXrVLFiRUlSq1at9PHHH2vixImaMGGCQkNDNX78eE2ePLm0Dxm45zkYhmGUdRP3iszMTHl6eiojI0MeHh6WbvuDuKOWbq+0jPpbvbJuAeXUtWvXdOLECdWqVUuurq5l3c5dh/lFiSrtXz0oUEL/h+t2P785UwUAAG5WVsHIjnFNFQAAgAUIVQAAABYgVAEAAFiAUAWgXONempLBvALWI1QBKJcKHhlw9erVMu7k7lQwrwXzDODOcfcfgHLJyclJXl5eOnv2rCTJ3d1dDg4OZdyV/TMMQ1evXtXZs2fl5eUlJyensm4JuGsQqgCUW35+fpJkBitYx8vLy5xfANYgVAEotxwcHOTv7y8fHx/l5uaWdTt3jYoVK3KGCigBhCoA5Z6TkxMhAEC5R6gqorlz52ratGlKS0tTkyZN9OGHH6pFixZl3ZbdupOf1+EnbgDgT/BU9FLF3X9FsGzZMkVFRWnixInavXu3mjRporCwMK73AAAA/KByUbRs2VKPPPKI5syZI0nKz89XYGCgXnrpJY0dO/ZP1+cHlcsPznIBsBucbbp9/KCyfcjJyVFycrLGjfu/vzBHR0eFhoYqKSmp0HWys7OVnZ1tvs/IyJD021+O1a5lXbF8m3ez6FW7y7oFuxLxeJ2ybgF3ky3Ty7oD3K1K4PP1t83+tt0/Ow9FqLpNv/zyi/Ly8uTr62uz3NfXV0eOHCl0nejoaL355ps3LQ8MDCyRHoGS8lpZNwAAt+WtEt365cuX5enpectxQlUJGjdunKKiosz3+fn5unDhgqpVq2bpQwwzMzMVGBioU6dOWf61Imwx16WDeS4dzHPpYJ5LT0nNtWEYunz5sgICAv6wjlB1m6pXry4nJyelp6fbLE9PT7/lA/RcXFzk4uJis8zLy6ukWpSHhwf/wZYS5rp0MM+lg3kuHcxz6SmJuf6jM1QFuPvvNjk7O6t58+aKj483l+Xn5ys+Pl4hISFl2BkAACgPOFNVBFFRURo8eLCCg4PVokULzZw5U1lZWXrmmWfKujUAAFDGCFVF0Lt3b507d04TJkxQWlqamjZtqtjY2JsuXi9tLi4umjhx4k1fNcJ6zHXpYJ5LB/NcOpjn0lPWc81zqgAAACzANVUAAAAWIFQBAABYgFAFAABgAUIVAACABQhVdmLu3LmqWbOmXF1d1bJlS+3YseMP61esWKEGDRrI1dVVjRs31rp160qpU/tXlLn++OOP1a5dO3l7e8vb21uhoaF/+neD3xT133SBpUuXysHBQd27dy/ZBu8SRZ3nS5cuKSIiQv7+/nJxcVG9evX434/bUNR5njlzpurXry83NzcFBgZq1KhRunbtWil1a5+2bNmirl27KiAgQA4ODlq1atWfrpOQkKBmzZrJxcVFderUUUxMTMk2aaDcW7p0qeHs7Gx89tlnxsGDB41hw4YZXl5eRnp6eqH1iYmJhpOTkzF16lTj0KFDxvjx442KFSsa+/fvL+XO7U9R57pfv37G3LlzjT179hiHDx82hgwZYnh6eho//fRTKXduX4o6zwVOnDhh3H///Ua7du2Mbt26lU6zdqyo85ydnW0EBwcbnTt3NrZu3WqcOHHCSEhIMFJSUkq5c/tS1HlevHix4eLiYixevNg4ceKEsX79esPf398YNWpUKXduX9atW2e8/vrrxldffWVIMlauXPmH9cePHzfc3d2NqKgo49ChQ8aHH35oODk5GbGxsSXWI6HKDrRo0cKIiIgw3+fl5RkBAQFGdHR0ofVPP/200aVLF5tlLVu2NJ577rkS7fNuUNS5/r3r168bVapUMRYtWlRSLd4VijPP169fN1q3bm188sknxuDBgwlVt6Go8zxv3jzjoYceMnJyckqrxbtCUec5IiLCePzxx22WRUVFGW3atCnRPu8mtxOqxowZY/zlL3+xWda7d28jLCysxPri679yLicnR8nJyQoNDTWXOTo6KjQ0VElJSYWuk5SUZFMvSWFhYbesx2+KM9e/d/XqVeXm5qpq1aol1abdK+48v/XWW/Lx8VF4eHhptGn3ijPPq1evVkhIiCIiIuTr66tGjRrp3XffVV5eXmm1bXeKM8+tW7dWcnKy+RXh8ePHtW7dOnXu3LlUer5XlMVnIU9UL+d++eUX5eXl3fTUdl9fXx05cqTQddLS0gqtT0tLK7E+7wbFmevf+8c//qGAgICb/kPG/ynOPG/dulWffvqpUlJSSqHDu0Nx5vn48ePauHGj+vfvr3Xr1um7777Tiy++qNzcXE2cOLE02rY7xZnnfv366ZdfflHbtm1lGIauX7+u559/Xq+99lpptHzPuNVnYWZmpn799Ve5ublZvk/OVAEWmTJlipYuXaqVK1fK1dW1rNu5a1y+fFkDBw7Uxx9/rOrVq5d1O3e1/Px8+fj4aMGCBWrevLl69+6t119/XfPnzy/r1u4qCQkJevfdd/XRRx9p9+7d+uqrr7R27VpNnjy5rFvDHeJMVTlXvXp1OTk5KT093WZ5enq6/Pz8Cl3Hz8+vSPX4TXHmusD777+vKVOm6JtvvtHDDz9ckm3avaLO8/fff68ffvhBXbt2NZfl5+dLkipUqKDU1FTVrl27ZJu2Q8X59+zv76+KFSvKycnJXNawYUOlpaUpJydHzs7OJdqzPSrOPL/xxhsaOHCghg4dKklq3LixsrKyNHz4cL3++utydOR8hxVu9Vno4eFRImepJM5UlXvOzs5q3ry54uPjzWX5+fmKj49XSEhIoeuEhITY1EtSXFzcLevxm+LMtSRNnTpVkydPVmxsrIKDg0ujVbtW1Hlu0KCB9u/fr5SUFPP1P//zP+rQoYNSUlIUGBhYmu3bjeL8e27Tpo2+++47M7RK0tGjR+Xv70+guoXizPPVq1dvCk4FQdbg53gtUyafhSV2CTwss3TpUsPFxcWIiYkxDh06ZAwfPtzw8vIy0tLSDMMwjIEDBxpjx4416xMTE40KFSoY77//vnH48GFj4sSJPFLhNhV1rqdMmWI4Ozsb//rXv4wzZ86Yr8uXL5fVIdiFos7z73H33+0p6jyfPHnSqFKlihEZGWmkpqYaa9asMXx8fIy33367rA7BLhR1nidOnGhUqVLF+PLLL43jx48bGzZsMGrXrm08/fTTZXUIduHy5cvGnj17jD179hiSjBkzZhh79uwxfvzxR8MwDGPs2LHGwIEDzfqCRyq8+uqrxuHDh425c+fySAX85sMPPzQefPBBw9nZ2WjRooXx7bffmmOPPfaYMXjwYJv65cuXG/Xq1TOcnZ2Nv/zlL8batWtLuWP7VZS5rlGjhiHpptfEiRNLv3E7U9R/0zciVN2+os7ztm3bjJYtWxouLi7GQw89ZLzzzjvG9evXS7lr+1OUec7NzTUmTZpk1K5d23B1dTUCAwONF1980bh48WLpN25HNm3aVOj/3hbM7eDBg43HHnvspnWaNm1qODs7Gw899JCxcOHCEu3RwTA41wgAAHCnuKYKAADAAoQqAAAACxCqAAAALECoAgAAsAChCgAAwAKEKgAAAAsQqgAAACxAqAIAALAAoQqAXRsyZIi6d+9+R9v44Ycf5ODgoJSUlFvWJCQkyMHBQZcuXZIkxcTEyMvLyxyfNGmSmjZtekd9ALBvhCoApWLIkCFycHCQg4ODnJ2dVadOHb311lu6fv16Wbd2W1q3bq0zZ87I09Oz0PHRo0fb/HirFWEPgH2pUNYNALh3dOrUSQsXLlR2drbWrVuniIgIVaxYUePGjbupNicnR87OzmXQZeGcnZ3l5+d3y/HKlSurcuXKpdiR9QzDUF5enipU4KMBKA7OVAEoNS4uLvLz81ONGjX0wgsvKDQ0VKtXr5b0f2d23nnnHQUEBKh+/fqSpP379+vxxx+Xm5ubqlWrpuHDh+vKlSs3bfvNN9/UfffdJw8PDz3//PPKyckxx2JjY9W2bVt5eXmpWrVqeuqpp/T999/ftI0jR46odevWcnV1VaNGjbR582Zz7Pdf//3ejV//TZo0SYsWLdJ//vMf8+xcQkKCHn/8cUVGRtqsd+7cOTk7O9uc5brR3r171aFDB1WpUkUeHh5q3ry5du3aZY4nJiaqffv2cnd3l7e3t8LCwnTx4kVJUnZ2tl5++WX5+PjI1dVVbdu21c6dO286pq+//lrNmzeXi4uLtm7dqvz8fEVHR6tWrVpyc3NTkyZN9K9//avQ/gD8H0IVgDLj5uZmE37i4+OVmpqquLg4rVmzRllZWQoLC5O3t7d27typFStW6JtvvrkpmMTHx+vw4cNKSEjQl19+qa+++kpvvvmmOZ6VlaWoqCjt2rVL8fHxcnR0VI8ePZSfn2+znVdffVWvvPKK9uzZo5CQEHXt2lXnz58v8nGNHj1aTz/9tDp16qQzZ87ozJkzat26tYYOHaolS5YoOzvbrP3iiy90//336/HHHy90W/3799cDDzygnTt3Kjk5WWPHjlXFihUlSSkpKerYsaOCgoKUlJSkrVu3qmvXrsrLy5MkjRkzRv/+97+1aNEi7d69W3Xq1FFYWJguXLhgs4+xY8dqypQpOnz4sB5++GFFR0fr888/1/z583Xw4EGNGjVKAwYMsAmZAAphAEApGDx4sNGtWzfDMAwjPz/fiIuLM1xcXIzRo0eb476+vkZ2dra5zoIFCwxvb2/jypUr5rK1a9cajo6ORlpamrle1apVjaysLLNm3rx5RuXKlY28vLxCezl37pwhydi/f79hGIZx4sQJQ5IxZcoUsyY3N9d44IEHjPfee88wDMPYtGmTIcm4ePGiYRiGsXDhQsPT09OsnzhxotGkSZNCj7fAr7/+anh7exvLli0zlz388MPGpEmTbjlvVapUMWJiYgod69u3r9GmTZtCx65cuWJUrFjRWLx4sbksJyfHCAgIMKZOnWpzTKtWrTJrrl27Zri7uxvbtm2z2V54eLjRt2/fW/YJwDA4UwWg1KxZs0aVK1eWq6urnnzySfXu3VuTJk0yxxs3bmxzHdXhw4fVpEkTVapUyVzWpk0b5efnKzU11VzWpEkTubu7m+9DQkJ05coVnTp1SpJ07Ngx9e3bVw899JA8PDxUs2ZNSdLJkydt+gsJCTH/XKFCBQUHB+vw4cOWHLskubq6auDAgfrss88kSbt379aBAwc0ZMiQW64TFRWloUOHKjQ0VFOmTLH52rLgTFVhvv/+e+Xm5qpNmzbmsooVK6pFixY3HVNwcLD55++++05Xr17V3/72N/M6scqVK+vzzz8v9CtTAP+HqxEBlJoOHTpo3rx5cnZ2VkBAwE0XRN8YnqzUtWtX1ahRQx9//LECAgKUn5+vRo0a2Xz1WFqGDh2qpk2b6qefftLChQv1+OOPq0aNGresnzRpkvr166e1a9fq66+/1sSJE7V06VL16NFDbm5ulvR047wXXK+2du1a3X///TZ1Li4uluwPuFtxpgpAqalUqZLq1KmjBx988LbuMGvYsKH27t2rrKwsc1liYqIcHR3NC9ml3y7m/vXXX8333377rSpXrqzAwECdP39eqampGj9+vDp27KiGDRuaF3L/3rfffmv++fr160pOTlbDhg2Lc6hydnY2r226UePGjRUcHKyPP/5YS5Ys0bPPPvun26pXr55GjRqlDRs2qGfPnlq4cKEk6eGHH77lBe61a9eWs7OzEhMTzWW5ubnauXOngoKCbrmvoKAgubi46OTJk6pTp47NKzAw8E97Be5lhCoA5Vb//v3l6uqqwYMH68CBA9q0aZNeeuklDRw4UL6+vmZdTk6OwsPDdejQIa1bt04TJ05UZGSkHB0d5e3trWrVqmnBggX67rvvtHHjRkVFRRW6v7lz52rlypU6cuSIIiIidPHixdsKPYWpWbOm9u3bp9TUVP3yyy/Kzc01x4YOHaopU6bIMAz16NHjltv49ddfFRkZqYSEBP34449KTEzUzp07zaA3btw47dy5Uy+++KL27dunI0eOaN68efrll19UqVIlvfDCC3r11VcVGxurQ4cOadiwYbp69arCw8Nvuc8qVapo9OjRGjVqlBYtWqTvv/9eu3fv1ocffqhFixYVay6AewVf/wEot9zd3bV+/XqNGDFCjzzyiNzd3dWrVy/NmDHDpq5jx46qW7euHn30UWVnZ6tv377mtVqOjo5aunSpXn75ZTVq1Ej169fX7Nmz1b59+5v2N2XKFE2ZMkUpKSmqU6eOVq9ererVqxer92HDhikhIUHBwcG6cuWKNm3aZO6zb9++GjlypPr27StXV9dbbsPJyUnnz5/XoEGDlJ6erurVq6tnz57mnY316tXThg0b9Nprr6lFixZyc3NTy5Yt1bdvX/N48vPzNXDgQF2+fFnBwcFav369vL29/7D3yZMn67777lN0dLSOHz8uLy8vNWvWTK+99lqx5gK4VzgYhmGUdRMAcC/54YcfVLt2be3cuVPNmjUr63YAWIRQBQClJDc3V+fPn9fo0aN14sQJm+udANg/rqkCgFKSmJgof39/7dy5U/Pnzy/rdgBYjDNVAAAAFuBMFQAAgAUIVQAAABYgVAEAAFiAUAUAAGABQhUAAIAFCFUAAAAWIFQBAABYgFAFAABggf8PfWJGDVtPwbAAAAAASUVORK5CYII=",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# print('number of points:', dataset[1].get('point').shape[0])\n",
    "\n",
    "scores = result['predict_scores']\n",
    "\n",
    "plt.hist(scores[:, 0], alpha=0.5, bins=20, label='leaf')\n",
    "plt.hist(scores[:, 1], alpha=0.5, bins=20, label='wood')\n",
    "\n",
    "# Add labels and legend\n",
    "plt.xlabel('Probability score')\n",
    "plt.ylabel('Frequency')\n",
    "plt.legend()\n",
    "\n",
    "# Show the plot\n",
    "plt.show()\n",
    "\n",
    "labels = result['predict_labels']\n",
    "\n",
    "plt.hist(labels, alpha=0.5, label='Column 1')\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./test\n",
      "[0 0 0 ... 0 0 0]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "test 0/30: 100%|| 245795/245795 [00:04<00:00, 60539.12it/s]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./test\n",
      "[1 1 1 ... 0 0 0]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "test 1/30: 100%|| 80440/80440 [00:00<00:00, 146766.65it/s]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./test\n",
      "[1 1 1 ... 0 0 0]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "test 2/30: 100%|| 116217/116217 [00:01<00:00, 100078.54it/s]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./test\n",
      "[1 1 1 ... 1 1 1]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "test 3/30: 100%|| 46015/46015 [00:00<00:00, 98833.97it/s] \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./test\n",
      "[1 1 1 ... 0 0 0]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "test 4/30: 100%|| 129759/129759 [00:01<00:00, 104577.01it/s]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./test\n",
      "[0 0 0 ... 0 0 0]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "test 5/30: 100%|| 75896/75896 [00:00<00:00, 106444.11it/s]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./test\n",
      "[1 1 1 ... 0 0 0]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "test 6/30: 100%|| 57590/57590 [00:00<00:00, 119257.49it/s]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./test\n",
      "[1 1 1 ... 0 0 0]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "test 7/30: 100%|| 198300/198300 [00:02<00:00, 76859.05it/s] \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./test\n",
      "[1 1 1 ... 1 1 1]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "test 8/30: 100%|| 178852/178852 [00:02<00:00, 60158.47it/s]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./test\n",
      "[0 0 0 ... 0 0 0]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "test 9/30: 100%|| 223008/223008 [00:04<00:00, 53789.48it/s]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./test\n",
      "[1 1 1 ... 1 1 1]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "test 10/30: 100%|| 97371/97371 [00:01<00:00, 62885.31it/s]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./test\n",
      "[0 0 0 ... 0 0 0]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "test 11/30: 100%|| 232799/232799 [00:03<00:00, 66603.96it/s]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./test\n",
      "[1 1 1 ... 1 1 1]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "test 12/30: 100%|| 30275/30275 [00:00<00:00, 70074.73it/s] \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./test\n",
      "[1 1 1 ... 0 0 0]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "test 13/30: 100%|| 42344/42344 [00:00<00:00, 89475.56it/s] \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./test\n",
      "[0 0 0 ... 0 0 0]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "test 14/30: 100%|| 642825/642825 [00:11<00:00, 54799.23it/s]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./test\n",
      "[1 1 1 ... 1 1 1]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "test 15/30: 100%|| 25296/25296 [00:00<00:00, 46720.12it/s]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./test\n",
      "[1 1 1 ... 1 1 1]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "test 16/30: 100%|| 82954/82954 [00:00<00:00, 110881.57it/s]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./test\n",
      "[0 0 0 ... 0 0 0]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "test 17/30: 100%|| 69426/69426 [00:00<00:00, 133278.17it/s]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./test\n",
      "[1 1 1 ... 1 1 1]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "test 18/30: 100%|| 105828/105828 [00:01<00:00, 61035.65it/s] \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./test\n",
      "[1 1 1 ... 1 1 1]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "test 19/30: 100%|| 48616/48616 [00:00<00:00, 101193.61it/s]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./test\n",
      "[1 1 1 ... 1 1 1]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "test 20/30: 100%|| 56575/56575 [00:00<00:00, 109880.14it/s]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./test\n",
      "[0 0 0 ... 0 0 0]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "test 21/30: 100%|| 431388/431388 [00:07<00:00, 60823.43it/s]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./test\n",
      "[1 1 1 ... 0 0 0]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "test 22/30: 100%|| 476184/476184 [00:08<00:00, 56418.05it/s]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./test\n",
      "[0 0 0 ... 1 1 1]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "test 23/30: 100%|| 62992/62992 [00:00<00:00, 92785.88it/s] \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./test\n",
      "[1 1 1 ... 0 0 0]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "test 24/30: 100%|| 65180/65180 [00:00<00:00, 127186.16it/s]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./test\n",
      "[1 1 1 ... 0 0 0]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "test 25/30: 100%|| 452501/452501 [00:07<00:00, 57617.53it/s]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./test\n",
      "[1 1 1 ... 1 1 1]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "test 26/30: 100%|| 503084/503084 [00:08<00:00, 57794.12it/s]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./test\n",
      "[1 1 1 ... 0 0 0]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "test 27/30: 100%|| 153977/153977 [00:02<00:00, 59276.56it/s]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./test\n",
      "[0 0 0 ... 0 0 0]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "test 28/30: 100%|| 1864862/1864862 [00:39<00:00, 46961.98it/s]\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./test\n",
      "[1 1 1 ... 1 1 1]\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "'NoneType' object is not subscriptable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[45], line 40\u001b[0m\n\u001b[1;32m     37\u001b[0m ckpt_path \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39m../logs/CustomRandLANet_TropicalLeafWood_torch/checkpoint/ckpt_00040.pth\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m     38\u001b[0m pipeline\u001b[39m.\u001b[39mload_ckpt(ckpt_path)\n\u001b[0;32m---> 40\u001b[0m pipeline\u001b[39m.\u001b[39;49mrun_test()\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/open3d/_ml3d/torch/pipelines/semantic_segmentation.py:259\u001b[0m, in \u001b[0;36mSemanticSegmentation.run_test\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    255\u001b[0m             \u001b[39mif\u001b[39;00m \u001b[39m'\u001b[39m\u001b[39mtest\u001b[39m\u001b[39m'\u001b[39m \u001b[39min\u001b[39;00m record_summary \u001b[39mand\u001b[39;00m \u001b[39m'\u001b[39m\u001b[39mtest\u001b[39m\u001b[39m'\u001b[39m \u001b[39mnot\u001b[39;00m \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39msummary:\n\u001b[1;32m    256\u001b[0m                 \u001b[39mself\u001b[39m\u001b[39m.\u001b[39msummary[\u001b[39m'\u001b[39m\u001b[39mtest\u001b[39m\u001b[39m'\u001b[39m] \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mget_3d_summary(\n\u001b[1;32m    257\u001b[0m                     results, inputs[\u001b[39m'\u001b[39m\u001b[39mdata\u001b[39m\u001b[39m'\u001b[39m], \u001b[39m0\u001b[39m, save_gt\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m)\n\u001b[1;32m    258\u001b[0m log\u001b[39m.\u001b[39minfo(\n\u001b[0;32m--> 259\u001b[0m     \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mOverall Testing Accuracy : \u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmetric_test\u001b[39m.\u001b[39;49macc()[\u001b[39m-\u001b[39;49m\u001b[39m1\u001b[39;49m]\u001b[39m}\u001b[39;00m\u001b[39m, mIoU : \u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmetric_test\u001b[39m.\u001b[39miou()[\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m]\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m\n\u001b[1;32m    260\u001b[0m )\n\u001b[1;32m    262\u001b[0m log\u001b[39m.\u001b[39minfo(\u001b[39m\"\u001b[39m\u001b[39mFinished testing\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "\u001b[0;31mTypeError\u001b[0m: 'NoneType' object is not subscriptable"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A\u001b[A"
     ]
    }
   ],
   "source": [
    "class CustomDataset(Custom3D):\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "\n",
    "    @staticmethod\n",
    "    def get_label_to_names():\n",
    "        label_to_names = {\n",
    "            0: 'leaf',\n",
    "            1: 'wood',\n",
    "        }\n",
    "        return label_to_names\n",
    "    \n",
    "    def save_test_result(self, results, attr):\n",
    "        cfg = self.cfg\n",
    "        name = attr['name']\n",
    "        path = cfg.test_result_folder\n",
    "        if not os.path.exists(path):\n",
    "            os.mkdir(path)\n",
    "        \n",
    "        pred = results['predict_labels']\n",
    "\n",
    "        store_path = os.path.join(path, name + '.npy')\n",
    "        np.save(store_path, pred)\n",
    "\n",
    "\n",
    "# Load config file\n",
    "cfg_path = \"../cfg/randlanet_leafwood.yml\"\n",
    "cfg = _ml3d.utils.Config.load_from_file(cfg_path)\n",
    "\n",
    "dataset = CustomDataset(**cfg.dataset)\n",
    "model = CustomRandLANet(**cfg.model)\n",
    "pipeline = ml3d.pipelines.SemanticSegmentation(model, dataset, **cfg.pipeline)\n",
    "\n",
    "ckpt_path = \"../logs/CustomRandLANet_TropicalLeafWood_torch/checkpoint/ckpt_00040.pth\"\n",
    "pipeline.load_ckpt(ckpt_path)\n",
    "\n",
    "pipeline.run_test()\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "spacetwin",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "d613a8f84a3cca9fdff0d15341a4965998d98e6aa842c40eaa5326b105ac6f44"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
