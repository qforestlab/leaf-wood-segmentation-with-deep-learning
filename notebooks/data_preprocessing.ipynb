{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Data preprocessing\n",
    "\n",
    "\n",
    "Code used for preprocessing the dataset presented in the paper \"Pointwise deep learning for leaf-wood segmentation of tropical tree point clouds from terrestrial laser scanning\"  \n",
    "\n",
    "--- "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Jupyter environment detected. Enabling Open3D WebVisualizer.\n",
      "[Open3D INFO] WebRTC GUI backend enabled.\n",
      "[Open3D INFO] WebRTCWindowSystem: HTTP handshake server disabled.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import open3d as o3d\n",
    "# import open3d.ml as _ml3d\n",
    "# import open3d.ml.torch as ml3d\n",
    "import ml3d as _ml3d\n",
    "import ml3d.torch as ml3d\n",
    "from open3d.ml.torch.datasets import Custom3D\n",
    "import numpy as np\n",
    "import glob\n",
    "import torch\n",
    "from pclbox.models import CustomRandLANet\n",
    "\n",
    "DATA_DIR = \"/mnt/c/Users/wavdnbro/OneDrive - UGent/Documents/spacetwin/datasets/leaf_wood/\"\n",
    "DATA_PATH = DATA_DIR + 'preprocessed/'"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocess dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------------\n",
    "# Code to preprocess the tropical leaf-wood data of Louise Terryn\n",
    "#\n",
    "# The manually labeled wood points did not match the original point cloud due to precision mismatch when saving the files.\n",
    "# I thus saved the original point clouds with the same precision (%.3f). Then, I attributed a label to each point in the \n",
    "# original point cloud indicating whether the point is 'wood' or 'non-wood'. \n",
    "# \n",
    "# ---------------\n",
    "\n",
    "PATH_TREE = os.path.join(DATA_DIR, 'tree_points')\n",
    "PATH_WOOD = os.path.join(DATA_DIR, 'wood_points')\n",
    "plot_names = ['DRO', 'OC', 'RC']\n",
    "\n",
    "# Create different variables holding the filenames and paths\n",
    "filenames = {plot_name: os.listdir(os.path.join(PATH_TREE, plot_name)) for plot_name in plot_names}\n",
    "\n",
    "filenames_all = []\n",
    "for plot_name in plot_names:\n",
    "    filenames_all = filenames_all + filenames[plot_name]\n",
    "\n",
    "filepaths_tree = []\n",
    "for plot_name in plot_names:\n",
    "    filepaths_tree = filepaths_tree + [os.path.join(PATH_TREE, plot_name, filename) for filename in filenames[plot_name]]\n",
    "\n",
    "filepaths_wood = []\n",
    "for plot_name in plot_names:\n",
    "    filepaths_wood = filepaths_wood + [os.path.join(PATH_WOOD, plot_name, filename[:-6] + 'tls_0.02_wood.txt') for filename in filenames[plot_name]]\n",
    "\n",
    "\n",
    "def decrease_precision():\n",
    "\n",
    "    for file_in, filename_out in zip(filepaths_tree, filenames_all):\n",
    "        # Read tree\n",
    "        tree = np.loadtxt(file_in)   \n",
    "\n",
    "        if not os.path.exists(os.path.join(DATA_DIR, 'tmp')):\n",
    "                    os.makedirs(os.path.join(DATA_DIR, 'tmp'))\n",
    "\n",
    "        # Write file with lower precision\n",
    "        path_out = os.path.join(DATA_DIR, 'tmp', filename_out)\n",
    "        np.savetxt(path_out, tree, fmt='%.3f')\n",
    "\n",
    "\n",
    "def view1D(a, b): # a, b are arrays\n",
    "    a = np.ascontiguousarray(a)\n",
    "    b = np.ascontiguousarray(b)\n",
    "    void_dt = np.dtype((np.void, a.dtype.itemsize * a.shape[1]))\n",
    "    return a.view(void_dt).ravel(),  b.view(void_dt).ravel()\n",
    "\n",
    "\n",
    "def isin_nd(a,b):\n",
    "    # a,b are the 3D input arrays to give us \"isin-like\" functionality across them\n",
    "    A,B = view1D(a.reshape(a.shape[0],-1),b.reshape(b.shape[0],-1))\n",
    "    return np.isin(A,B)\n",
    "\n",
    "\n",
    "def add_label():\n",
    "\n",
    "     for filename, path_wood in zip(filenames_all, filepaths_wood):\n",
    "        # Read tree\n",
    "        tree = np.loadtxt(os.path.join(DATA_DIR, 'tmp', filename)) \n",
    "        wood = np.loadtxt(path_wood) \n",
    "        wood = wood[:, :3]\n",
    "\n",
    "        label = isin_nd(tree, wood)\n",
    "        file_out = np.hstack((tree, label.reshape(-1, 1)))\n",
    "     \n",
    "        if not os.path.exists(os.path.join(DATA_DIR, 'preprocessed')):\n",
    "            os.makedirs(os.path.join(DATA_DIR, 'preprocessed'))\n",
    "     \n",
    "        # Write file with label\n",
    "        path_out = os.path.join(DATA_DIR, 'preprocessed', filename)\n",
    "        np.savetxt(path_out, file_out, fmt='%.3f')\n",
    "\n",
    "\n",
    "# decrease_precision()\n",
    "# add_label()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Preprocess data to the file structure necessary for open3d ml datasets\n",
    "import random\n",
    "\n",
    "def convert_dataset():\n",
    "    DATA_PATH_IN = os.path.join(DATA_DIR, 'preprocessed') \n",
    "    DATA_PATH_OUT = os.path.join(DATA_DIR, 'preprocessed_open3d') \n",
    "\n",
    "    # Make required file structure\n",
    "    path_train = os.path.join(DATA_PATH_OUT, 'train')\n",
    "    path_val = os.path.join(DATA_PATH_OUT, 'val')\n",
    "    path_test = os.path.join(DATA_PATH_OUT, 'test')\n",
    "\n",
    "    for path in [path_train, path_val, path_test]:\n",
    "        if not os.path.exists(path):\n",
    "            os.makedirs(path)\n",
    "\n",
    "    # Get all filenames of point clouds\n",
    "    filenames = os.listdir(DATA_PATH_IN)\n",
    "\n",
    "    # Randomly shuffle the filenames\n",
    "    random.seed(42)\n",
    "    random.shuffle(filenames)\n",
    "\n",
    "    # Define train-val-test split\n",
    "    n_files = len(filenames)\n",
    "    split_train = 0.6\n",
    "    split_test = 0.2\n",
    "\n",
    "    # Get train-val-test files \n",
    "    files_train = filenames[:round(split_train*n_files)]\n",
    "    files_val = filenames[round(split_train*n_files):round((split_train + split_test)*n_files)]\n",
    "    files_test = filenames[round((split_train + split_test)*n_files):]\n",
    "\n",
    "\n",
    "    for files, path_out in zip([files_train, files_val], [path_train, path_val]):\n",
    "        for file in files:\n",
    "            # Read file\n",
    "            pcl = np.loadtxt(os.path.join(DATA_PATH_IN, file)) \n",
    "            # Write file\n",
    "            filename_out = os.path.join(path_out, file[:-3] + 'npy')\n",
    "            with open(filename_out, 'wb') as f:\n",
    "                np.save(f, pcl)\n",
    "\n",
    "    for file in files_test:\n",
    "        # Read file\n",
    "        pcl = np.loadtxt(os.path.join(DATA_PATH_IN, file)) \n",
    "\n",
    "        # Only retain xyz\n",
    "        xyz = pcl[:, :3]\n",
    "        labels = pcl[:, 3].astype(np.uint8)\n",
    "        \n",
    "        # Write file\n",
    "        filename_out = os.path.join(path_test, file[:-3] + 'npy')\n",
    "        with open(filename_out, 'wb') as f:\n",
    "            np.save(f, xyz)\n",
    "\n",
    "        filename_labels = os.path.join(path_test, file[:-4] + '_labels.txt')\n",
    "        np.savetxt(filename_labels, labels, fmt='%1.i')\n",
    "\n",
    "convert_dataset()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of trees: 60\n",
      "total number of points: 14364895\n",
      "average number of points per tree: 239414.91666666666\n",
      "max number of points: 1887667\n",
      "min number of points: 25483\n",
      "total number of woody points: 0\n",
      "total_number of non-wood points: 14364895\n",
      "fraction: 0.0\n"
     ]
    }
   ],
   "source": [
    "DATA_DIR = \"/mnt/c/Users/wavdnbro/OneDrive - UGent/Documents/spacetwin/datasets/leaf_wood/\"\n",
    "DATA_PATH = DATA_DIR + 'preprocessed_open3d/test/'\n",
    "\n",
    "n_trees = len(os.listdir(DATA_PATH))\n",
    "print('number of trees:', n_trees)\n",
    "\n",
    "total_points = 0\n",
    "wood_points = 0\n",
    "max_points = 0\n",
    "min_points = 100000000\n",
    "\n",
    "for filename in os.listdir(DATA_PATH):\n",
    "    # tree = np.load(DATA_PATH + filename)\n",
    "    if filename[-3:] == 'txt':\n",
    "        tree = np.loadtxt(DATA_PATH + filename) \n",
    "        # print(tree)\n",
    "    \n",
    "    total_points += len(tree)\n",
    "    # wood_points += tree[:, 3].sum()\n",
    "    # wood_points += tree.sum()\n",
    "    max_points = max(max_points, len(tree))\n",
    "    min_points = min(min_points, len(tree))\n",
    "\n",
    "\n",
    "print('total number of points:', total_points)\n",
    "print('average number of points per tree:', total_points / n_trees)\n",
    "print('max number of points:', max_points)\n",
    "print('min number of points:', min_points)\n",
    "print('total number of woody points:', wood_points)\n",
    "print('total_number of non-wood points:', total_points - wood_points)\n",
    "print('fraction:', wood_points / total_points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of species: 41\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv(\"/mnt/c/Users/wavdnbro/OneDrive - UGent/Documents/spacetwin/datasets/leaf_wood/AUS_samenvatting.csv\")\n",
    "\n",
    "print('number of species:', len(df.Species.unique()))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "spacetwin",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "d613a8f84a3cca9fdff0d15341a4965998d98e6aa842c40eaa5326b105ac6f44"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
